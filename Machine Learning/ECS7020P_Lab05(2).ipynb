{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IC6Bpw_gvTrw"
   },
   "source": [
    "# Lab 5: Exploring classification\n",
    "\n",
    "A classifier is a model that predicts a **discrete label** when shown a set of **predictors**. In machine learning, we build classifiers using datasets extracted from our target population. \n",
    "\n",
    "Classifiers can be defined as partitions of the **predictor space** into **decision regions**. A decision region is a region within the predictor space such that any sample that lies within it is assigned the same label. Decision regions are separated by **decision boundaries** and therefore, it is common to define classifiers using their decision boundaries, for instance:\n",
    "- If sample $x_i$ lies on one side of the boundary, it is classified as <font color=red> **O** </font>.\n",
    "- If $x_i$ lies on the other side, it is classified as <font color=blue> **O** </font>.\n",
    "\n",
    "Even though decision boundaries and regions define classifiers, not every classifier uses explicitely these notions to label samples. The kNN algorithm is one example of this: to classify a sample kNN doesn't look for the boundary, but the sample's k nearest neighbours.\n",
    "\n",
    "In this lab we will explore the basics of classification. We will first consider **linear classifiers**, which produce decision regions separated by linear boundaries. We will then use the **logistic function** to create a notion of classifier \"certainty\", which then we will use to define the **logistic regression classifier**.  We will finally implement two **optimisation approaches** to identify the logistic regression solution, namely **exhaustive search** and **gradient descent**. \n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_vDXuyrCvk8c"
   },
   "source": [
    "# Linear classifiers\n",
    "\n",
    "Linear classifiers partition the predictor space into decision regions separated by linear boundaries (e.g. straight lines). Let's illustrate linear classifiers using **a binary classifier** (two classes) in a **2D predictor space**.\n",
    "\n",
    "Let $x_1$ and $x_2$ be the predictors. A linear boundary in the predictor space is defined by the equation:\n",
    "\n",
    "$w_0 + w_1 x_1 + w_2 x_2 =0$,\n",
    "\n",
    "where $w_0$, $w_1$ and $w_2$ are the coefficients of the classifier. \n",
    "\n",
    "Using **vector notation** (this is much more convenient), we create the **extended predictor vector** $\\boldsymbol{x} = [1, x_1, x_2]^T$ and the **coefficients vector** $\\boldsymbol{w} = [w_0, w_1, w_2]^T$, and use them to express the linear boundary by the equivalent equation\n",
    "\n",
    "$\\boldsymbol{x}^T \\boldsymbol{w} = 0$.\n",
    "\n",
    "If you do not understand why the two previous equations are equivalent, please **revise the material on basic mathematical notation and matrix algebra now**. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GKq-SXZLW8S5"
   },
   "source": [
    "Consider the classifier defined by the coefficients vector $\\boldsymbol{w} = [-0.25, -1, 1]^T$. Can you plot it on a piece of paper? Try to do it before continuing to check your understanding. \n",
    "\n",
    "Notice that by making $\\boldsymbol{x}^T \\boldsymbol{w} = 0$, we get\n",
    "\n",
    "\\begin{eqnarray}\n",
    "\\boldsymbol{x}^T \\boldsymbol{w} &=& [1, x_1, x_2] [-0.25, -1, 1]^T \\\\\n",
    "&=& -0.25\\times 1 + (-1)\\times x_1 + 1\\times x_2 \\\\\n",
    "&=& -0.25 - x_1 + x_2 \\\\\n",
    "&=& 0, \\\\ \\\\\n",
    "\\rightarrow x_2 &=& x_1 + 0.25.\n",
    "\\end{eqnarray} \n",
    "\n",
    "Hence the boundary $\\boldsymbol{w} = [-0.25, -1, 1]^T$ is a straight line with a 45-degree slope and 0.25 intercept. \n",
    "\n",
    "Let's use the values 0 and 1 as labels for the two classes. Using numerical values for labels is very convenient from an implementation perspective. Our classifier will work as follows:\n",
    "\n",
    "- If $\\boldsymbol{x}_i^T \\boldsymbol{w} > 0$, then $\\hat{y}_i = 1$.\n",
    "- If $\\boldsymbol{x}_i^T \\boldsymbol{w} < 0$, then $\\hat{y}_i = 0$.\n",
    "\n",
    "Let's classify the following two samples:\n",
    "\n",
    "- First sample: $x_1 = 1$, $x_2 = 2$. \n",
    "- Second sample: $x_1 = 2$, $x_2 = 1$.\n",
    "\n",
    "You can do this easily by hand (try it!), but let's use the cell bellow:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "f1XweYWDW7BU"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "np.set_printoptions(precision=3)\n",
    "\n",
    "# This is our coefficients vector\n",
    "w = np.array([-0.25, -1, 1], ndmin=2).T\n",
    "\n",
    "# These are the two samples that we are considering\n",
    "firstsample = np.array([1, 1, 2], ndmin=2).T\n",
    "secondsample = np.array([1, 2, 1], ndmin=2).T\n",
    "\n",
    "# These are the predictions\n",
    "firstlabel = 1*(np.dot(firstsample.T,w).item() > 0)\n",
    "secondlabel = 1*(np.dot(secondsample.T,w).item() > 0)\n",
    "\n",
    "print('The predicted label of firstsample, where x1=', firstsample[1], 'and x2=', firstsample[2] ,', is ', firstlabel)\n",
    "print('The predicted label of secondsample, where x1=', secondsample[1], 'and x2=', secondsample[2] ,', is ', secondlabel)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oRF_9Y3fXAyv"
   },
   "source": [
    "We have used the trick of multiplying by 1 (`1*`) to convert the boolean values True and False to the numerical values 1 and 0, and `item()` to extract a scalar value from a (1,1) NumPy array. Go ahead and run the cell again without `item()` first and then without `1*` ans see what you obtain. It will be the **same result**, but **different representation**.\n",
    "\n",
    "To predict the label of a large number of samples, instead of operating on each sample separately, we can create a **design matrix** $\\boldsymbol{X}$ and multiply it by $\\boldsymbol{w}$. Note that in a design matrix, each row is the extended vector of one of the samples. A vector of predicted labels $\\boldsymbol{\\hat{y}}$ can then be obtained by comparing the result of the product $\\boldsymbol{X} \\boldsymbol{w}$ with 0.\n",
    "\n",
    "Let's do this for a collection of 25 samples on a rectangular grid in the range $-1 \\leq x_1 \\leq 1$ and $-1 \\leq x_2 \\leq 1$. We first build the grid using `meshgrid()` as follows:  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "N6C4_OgDvsCl"
   },
   "outputs": [],
   "source": [
    "# Here we create a dataset consiting of 25 samples on a rectangular grid\n",
    "coor1 = np.linspace(-1, 1, 5)\n",
    "coor2 = np.linspace(-1, 1, 5)\n",
    "\n",
    "x1_mesh, x2_mesh = np.meshgrid(coor1, coor2)\n",
    "\n",
    "x1 = x1_mesh.ravel()\n",
    "x2 = x2_mesh.ravel()\n",
    "\n",
    "print(\"My 25 grid samples are (x1, x2): \\n\", np.column_stack([x1, x2]))\n",
    "\n",
    "\n",
    "# The design matrix is created here\n",
    "X = np.column_stack([np.ones(x1.shape), x1, x2])\n",
    "\n",
    "print(\"My design matrix is \\n\", X)\n",
    "\n",
    "\n",
    "\n",
    "# Finally, we plot the 25 samples on the predictor space\n",
    "plt.scatter(x1, x2)\n",
    "plt.title(\"Samples in the predictor space\")\n",
    "plt.xlabel(\"x1\", fontsize=18)\n",
    "plt.ylabel(\"x2\", fontsize=18)\n",
    "plt.xlim(-1.5,1.5)\n",
    "plt.ylim(-1.5,1.5)\n",
    "plt.gca().set_aspect('equal', adjustable='box')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "45nArVRFcokt"
   },
   "source": [
    "Let's obtain and plot the labels predicted by the classifier $\\boldsymbol{w} = [-0.25, -1, 1]^T$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "E_qsK0kkdg3r"
   },
   "outputs": [],
   "source": [
    "# This line of code produces the predicted labels for all the samples in our dataset\n",
    "yPred = 1*(np.dot(X, w) > 0)\n",
    "\n",
    "\n",
    "# Plots each sample with a colour-coded (predicted) label\n",
    "scatter = plt.scatter(x1, x2, c=yPred, cmap=plt.get_cmap('bwr'))\n",
    "plt.title(\"Linear classifier and predicted labels\")\n",
    "plt.xlabel(\"x1\", fontsize=18)\n",
    "plt.ylabel(\"x2\", fontsize=18)\n",
    "plt.xlim(-1.5,1.5)\n",
    "plt.ylim(-1.5,1.5)\n",
    "plt.legend(handles=scatter.legend_elements()[0], labels=[\"0\", \"1\"])\n",
    "\n",
    "# Plots the decision boundary\n",
    "x1_L = np.linspace(-1.5, 1.5, 5)\n",
    "x2_L = x1_L + 0.25\n",
    "plt.plot(x1_L, x2_L)\n",
    "plt.gca().set_aspect('equal', adjustable='box')\n",
    "plt.grid(True)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RdAXNxfTooJw"
   },
   "source": [
    "We can represent the decision regions by evaluating a dense grid of points (100 $\\times$ 100 in the cell below) and plotting their predicted labels as an image using `imshow()`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "c1GW-yjqkq8t"
   },
   "outputs": [],
   "source": [
    "# Creates 100x100 samples on a rectangular grid, creates a design matrix and computes the predicted label\n",
    "coor1 = np.linspace(-1, 1, 100)\n",
    "coor2 = np.linspace(-1, 1, 100)\n",
    "x1_100, x2_100 = np.meshgrid(coor1, coor2)\n",
    "X = np.column_stack([np.ones(x1_100.ravel().shape), x1_100.ravel(), x2_100.ravel()])\n",
    "yPred = 1*(np.dot(X, w) > 0)\n",
    "\n",
    "\n",
    "# Plots the predicted labels for each sample as an image, allowing us to visualise the decision regions\n",
    "plt.imshow(yPred.reshape((100, 100)), \n",
    "                          cmap = 'bwr', \n",
    "                          interpolation='none', \n",
    "                          extent = (-1, 1, -1, 1), \n",
    "                          origin = 'lower', \n",
    "                          alpha = 0.9)\n",
    "\n",
    "plt.xlabel(\"x1\", fontsize=18)\n",
    "plt.ylabel(\"x2\", fontsize=18)\n",
    "plt.grid(True)\n",
    "plt.gca().set_aspect('equal', adjustable='box')\n",
    "plt.legend(handles=scatter.legend_elements()[0], labels=[\"Label 0\", \"Label 1\"])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UXZ7pdqfvsfD"
   },
   "source": [
    "# Logistic function: The certainty of a linear classifier\n",
    "\n",
    "Now that we know how to define, visualise and use a linear classifier, let's create the notion of classifier's certainty using the **logistic function** (also known as expit or sigmoid) $p(d)$:\n",
    "\n",
    "$$\n",
    "p(d) =\\frac{e^d}{e^d + 1} = \\frac{1}{1 + e^{-d}}\n",
    "$$\n",
    "\n",
    "Let's plot it first (we could easily implement it ourselves, but we will just use the `expit` function from the `scipy` scientific library):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WHqP6800Iy3a"
   },
   "outputs": [],
   "source": [
    "from scipy.special import expit\n",
    "\n",
    "d = np.linspace(-10, 10, 1000)\n",
    "\n",
    "plt.title(\"The logistic function\")\n",
    "plt.plot(d, expit(d), linewidth = 3)\n",
    "plt.xlabel('$d$', fontsize=16)\n",
    "plt.ylabel('$p(d)$', fontsize=16)\n",
    "plt.ylim(0, 1)\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ktxkiwh7JdTk"
   },
   "source": [
    "Notice that: \n",
    "- $p(d) \\rightarrow 1$ as $d \\rightarrow \\infty$, \n",
    "- $p(0) = 0.5$, \n",
    "- $p(d) \\rightarrow 0$ as $d \\rightarrow -\\infty$.\n",
    "\n",
    "In classification, the quantity $\\boldsymbol{x}^T\\boldsymbol{w}$ can be interpreted as a distance between the sample $\\boldsymbol{x}$ and the boundary. Setting $d=\\boldsymbol{x}^T\\boldsymbol{w}$, we will use the quantity $p(\\boldsymbol{x}^T\\boldsymbol{w})$ to measure a classifier's certainty that a sample $\\boldsymbol{x}$ belongs to one of the classes.\n",
    "\n",
    "We will define a new function `p(X, w)` corresponding to the quantity $p(\\boldsymbol{x}^T\\boldsymbol{w})$: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ooZ1HJkJuQ2s"
   },
   "outputs": [],
   "source": [
    "def p(X, w):\n",
    "  return expit(np.dot(X, w))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YJ2A5eUluRbz"
   },
   "source": [
    "Let's see how we can use the logistic function to define the notion of certainty in classifiers. We will first create a new linear classifier defined by the coefficients $\\boldsymbol{w} = [0.25, -1, 0]^T$ and show the decision regions that it defines:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lMILGT601QsD"
   },
   "outputs": [],
   "source": [
    "# Coefficients of the new classifier\n",
    "w = np.array([0.25, -1, 0], ndmin=2).T\n",
    "\n",
    "# 100x100 samples used to visualise the decision regions, design matrix and predicted labels for each sample\n",
    "coor1 = np.linspace(-2, 2, 100)\n",
    "coor2 = np.linspace(-2, 2, 100)\n",
    "x1, x2 = np.meshgrid(coor1, coor2)\n",
    "X = np.column_stack([np.ones(x1.ravel().shape), x1.ravel(), x2.ravel()])\n",
    "yPred = 1*(np.dot(X, w) > 0)\n",
    "\n",
    "# Shows decision regiones by showing as an image the predicted label of the 100x100 samples\n",
    "image = plt.imshow(yPred.reshape((100, 100)), \n",
    "                          cmap = 'bwr', \n",
    "                          interpolation='none', \n",
    "                          extent = (-2, 2, -2, 2), \n",
    "                          origin = 'lower', \n",
    "                          alpha = 0.9)\n",
    "\n",
    "plt.xlabel(\"x1\", fontsize=18)\n",
    "plt.ylabel(\"x2\", fontsize=18)\n",
    "plt.grid(True)\n",
    "plt.gca().set_aspect('equal', adjustable='box')\n",
    "plt.legend(handles=scatter.legend_elements()[0], labels=[\"Label 0\", \"Label 1\"])\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gYmhptPc5sRP"
   },
   "source": [
    "Would you have obtained the same decision regions by hand? Make sure that you are capable of identifying linear boundaries in a 2D predictor space using the coefficients vector $\\boldsymbol{w}$.\n",
    "\n",
    "Let's now consider 7 samples that **belong to class 1**. The quantity $p(\\boldsymbol{x}^T\\boldsymbol{w})$ corresponds to the classifier's certainty that each sample belongs to class 1. We can now plot the 7 sample along with the classifier's certainty for each one of them: \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "exjSCq8yJWjr"
   },
   "outputs": [],
   "source": [
    "# 7 samples\n",
    "x1 = np.array([1.5, 1, 0.5,  0, -0.5, -1, -1.5])\n",
    "x2 = np.array(np.zeros(x1.shape))\n",
    "\n",
    "# Classifier's certainty that each sample belongs to class 1\n",
    "X = np.column_stack([np.ones(x1.shape), x1, x2])\n",
    "certainty_1 = p(X, w)\n",
    "\n",
    "\n",
    "# Visualisation of the classifier, sample, and certainty values\n",
    "image = plt.imshow(yPred.reshape((100, 100)), \n",
    "                          cmap = 'bwr', \n",
    "                          interpolation='none', \n",
    "                          extent = (-2, 2, -2, 2), \n",
    "                          origin = 'lower', \n",
    "                          alpha = 0.2)\n",
    "\n",
    "plt.scatter(x1, x2, color = 'red')\n",
    "plt.xlabel(\"x1\", fontsize=18)\n",
    "plt.ylabel(\"x2\", fontsize=18)\n",
    "plt.xlim(-2,2)\n",
    "plt.ylim(-2,2)\n",
    "plt.gca().set_aspect('equal', adjustable='box')\n",
    "plt.grid(True)\n",
    "\n",
    "\n",
    "for i, c1 in enumerate(certainty_1):\n",
    "    plt.annotate(round(c1.item(),2), (x1[i], x2[i]))\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jG_Ir1P69b2H"
   },
   "source": [
    "If the same samples **belong to class 0 instead**, the classifier's certainty that they belong to class 0 is computed as $1 - p(\\boldsymbol{x}^T\\boldsymbol{w})$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jGVVVizVjbQq"
   },
   "outputs": [],
   "source": [
    "# Classifier's certainty that each sample belongs to 0\n",
    "certainty_0 = 1-p(X, w)\n",
    "\n",
    "# Visualisation of the classifier, samples and their certainties\n",
    "image = plt.imshow(yPred.reshape((100, 100)), \n",
    "                          cmap = 'bwr', \n",
    "                          interpolation='none', \n",
    "                          extent = (-2, 2, -2, 2), \n",
    "                          origin = 'lower', \n",
    "                          alpha = 0.2)\n",
    "\n",
    "plt.scatter(x1, x2, color = 'blue')\n",
    "plt.xlabel(\"x1\", fontsize=18)\n",
    "plt.ylabel(\"x2\", fontsize=18)\n",
    "plt.xlim(-2,2)\n",
    "plt.ylim(-2,2)\n",
    "plt.gca().set_aspect('equal', adjustable='box')\n",
    "plt.grid(True)\n",
    "\n",
    "for i, c1 in enumerate(certainty_0):\n",
    "    plt.annotate(round(c1.item(),2), (x1[i], x2[i]))\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UMPgvIHh92pn"
   },
   "source": [
    "As you can see, the certainty value for a sample that lies on the correct decision region is greater than 0.5, and increases towards 1 as we move away from the boundary. Samples that lie on the wrong decision region are assigned a certainty below 0.5, which decreases towards 0 as we move away from the boundary.\n",
    "\n",
    "In addition to computing the classifer's certainty for each individual sample, we can also do it for an entire dataset by simply multiplying the individual certainties. The resulting quantity $L$ is known as the **likelihood**:\n",
    "\n",
    "$$\n",
    "L=\\prod_{y_i=0}\\left(1-p(\\boldsymbol{x}_i^T \\boldsymbol{w})\\right) \\prod_{y_i=1}p(\\boldsymbol{x}_i^T \\boldsymbol{w})\n",
    "$$\n",
    "\n",
    "The **negative log-likelihood** is a more convenient way to quantify the classifier's certainty: \n",
    "\n",
    "$$\n",
    "-\\log L(\\boldsymbol{w})=-\\sum_{y_i=0}\\log\\left(1-p(\\boldsymbol{x}_i^T \\boldsymbol{w})\\right) -\\sum_{y_i=1}\\log p(\\boldsymbol{x}_i^T \\boldsymbol{w})\\\\\n",
    "= -\\sum_{y_i}[(1-y_i)\\log\\left(1-p(\\boldsymbol{x}_i^T \\boldsymbol{w})\\right) +y_i\\log p(\\boldsymbol{x}_i^T \\boldsymbol{w})]\n",
    "$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9nRS5bNYiVRd"
   },
   "source": [
    "In the following cell, we will define several new functions that implement the code for predicting labels and calculating the likelihood, negative log-likelihoods and accuracy of a classifier:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xtdmlSNgXiEi"
   },
   "outputs": [],
   "source": [
    "# Returns the predicted labels\n",
    "def prediction(X, w):\n",
    "  return 1*(np.dot(X, w) > 0)\n",
    "\n",
    "# Computes the likelihood\n",
    "def likelihood(X, w, y):\n",
    "  return np.prod(y*p(X, w) + (1-y)*(1-p(X, w)))\n",
    "\n",
    "# Computes the negative log-likelihood\n",
    "def negLogLikelihood(X, w, y):\n",
    "  return -np.sum(y * np.log(p(X,w)) + (1 - y) * np.log(1 - p(X,w)))\n",
    "\n",
    "# Computes the accuracy by comparing true labels y and predicted labels yP\n",
    "def accuracy(y, yP):\n",
    "  return np.sum(y==yP)/len(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "d7GvcFSeiSoH"
   },
   "source": [
    "Let's present two different datasets to our classifier and calculate the likelihood $L$, negative log-likelihood $l$, accuracy and error rate for each dataset. The following cell considers the first dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bb4Lu9fK91rq"
   },
   "outputs": [],
   "source": [
    "\n",
    "# 1st dataset consisting of 9 samples, x1 and x2 are the predictors, y is the TRUE label\n",
    "x1 = np.array([-1, 0, 1, -1,  0, 1, -1, 0, 1], ndmin=2).T\n",
    "x2 = np.array([1.5, 1.5, 1.5,  0, 0, 0, -1.5, -1.5, -1.5], ndmin=2).T\n",
    "y  = np.array([1, 1, 0, 1,  0, 0, 1, 1, 0], ndmin=2).T\n",
    "\n",
    "# Visualisation of the decision regions and samples. The colour of the samples corresponds to the TRUE LABEL\n",
    "image = plt.imshow(yPred.reshape((100, 100)), \n",
    "                          cmap = 'bwr', \n",
    "                          interpolation='none', \n",
    "                          extent = (-2, 2, -2, 2), \n",
    "                          origin = 'lower', \n",
    "                          alpha = 0.2)\n",
    "\n",
    "scatter = plt.scatter(x1, x2, c=y, cmap=plt.get_cmap('bwr'))\n",
    "\n",
    "plt.xlabel(\"x1\", fontsize = 14)\n",
    "plt.ylabel(\"x2\", fontsize = 14)\n",
    "plt.xlim(-2, 2)\n",
    "plt.ylim(-2, 2)\n",
    "plt.grid(alpha=0.2)\n",
    "plt.show()\n",
    "\n",
    "# Desing matrix, predicted label, certainties, likelihoods and negative log-likelihoods\n",
    "X = np.column_stack([np.ones(x1.shape), x1, x2])\n",
    "yP = prediction(X, w)\n",
    "certainty = y*p(X, w) + (1-y)*(1-p(X, w))\n",
    "L = likelihood(X, w, y)\n",
    "l = negLogLikelihood(X, w, y)\n",
    "\n",
    "print(\"The individual certainties for each sample are \\n\", certainty)\n",
    "print(\"The likelihood is L=\", L)\n",
    "print(\"The negative log-likelihood is -log(L)=\",l)\n",
    "\n",
    "# Accuracy and error rate\n",
    "A = accuracy(y, yP)\n",
    "E = 1 - A\n",
    "print(\"The accuracy is \", round(A, 2))\n",
    "print(\"The error rate is \", round(E,2))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OkUgsjgY_c_l"
   },
   "source": [
    "The cell bellow considers the second dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TZK0tGcpHgpn"
   },
   "outputs": [],
   "source": [
    "# 2nd dataset consisting of 9 samples\n",
    "\n",
    "x1 = np.array([1.5, 1.5, 1.5,  0, 0, 0, -1.5, -1.5, -1.5], ndmin=2).T\n",
    "x2 = np.array([-1, 0, 1, -1,  0, 1, -1, 0, 1], ndmin=2).T\n",
    "y  = np.array([1, 1, 0, 1,  0, 0, 1, 1, 0], ndmin=2).T\n",
    "\n",
    "# Visualisation of the decision regions and samples. The colour corresponds to the TRUE LABEL\n",
    "image = plt.imshow(yPred.reshape((100, 100)), \n",
    "                          cmap = 'bwr', \n",
    "                          interpolation='none', \n",
    "                          extent = (-2, 2, -2, 2), \n",
    "                          origin = 'lower', \n",
    "                          alpha = 0.2)\n",
    "\n",
    "scatter = plt.scatter(x1, x2, c=y, cmap=plt.get_cmap('bwr'))\n",
    "\n",
    "plt.xlabel(\"x1\", fontsize = 14)\n",
    "plt.ylabel(\"x2\", fontsize = 14)\n",
    "plt.xlim(-2, 2)\n",
    "plt.ylim(-2, 2)\n",
    "plt.grid(alpha=0.2)\n",
    "\n",
    "plt.show()\n",
    "\n",
    "# Desing matrix, predicted label, certainties, likelihoods and negative log-likelihoods\n",
    "X = np.column_stack([np.ones(x1.shape), x1, x2])\n",
    "yP = prediction(X, w)\n",
    "certainty = y*p(X, w) + (1-y)*(1-p(X, w))\n",
    "L = likelihood(X, w, y)\n",
    "l = negLogLikelihood(X, w, y)\n",
    "\n",
    "print(\"The individual certainties for each sample are \\n\", certainty)\n",
    "print(\"The likelihood is L=\", L)\n",
    "print(\"The negative log-likelihood is -log(L)=\", l)\n",
    "\n",
    "# Accuracy and error rate\n",
    "A = accuracy(y, yP)\n",
    "E = 1 - A\n",
    "print(\"The accuracy is \", round(A, 2))\n",
    "print(\"The error rate is \", round(E,2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ie9i48nvvyV7"
   },
   "source": [
    "Compare the likelihood, negative log-likelihood, accuracies and error rate values for both datasets. The classifier does a better job with the first dataset. Visually, it is clear that it separates the first dataset better. In addition, the classifier produces a higher likelihood (lower negative log-likelihood) and higher accuracy (lower error rate) when shown the first dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "M6dgLx2mdaV5"
   },
   "source": [
    "# Comparing classifiers\n",
    "\n",
    "In the previous section, we have shown two different datasets to the same classifier. We will now consider a single dataset and different linear classifiers. \n",
    "\n",
    "Upload the files `lab5_train.csv` and `lab5_test.csv`. Let's start by plotting the training dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-by3d0Ftl5HB"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Loads training and test datasets\n",
    "df_train = pd.read_csv(\"lab5_train.csv\")\n",
    "df_test = pd.read_csv(\"lab5_test.csv\")\n",
    "\n",
    "N = df_train['Label'].size\n",
    "\n",
    "# Stores each attribute in different (N,1) NumPy arrays\n",
    "x1_train = df_train['Attribute_1'].to_numpy().reshape(N,1)\n",
    "x2_train = df_train['Attribute_2'].to_numpy().reshape(N,1)\n",
    "y_train = df_train['Label'].to_numpy().reshape(N,1)\n",
    "\n",
    "\n",
    "\n",
    "# Plots the dataset\n",
    "fig = plt.figure(figsize=(7, 7))\n",
    "\n",
    "scatter = plt.scatter(x1_train, x2_train, c=y_train, cmap=plt.get_cmap('bwr'))\n",
    "plt.xlabel(\"x1\", fontsize = 14)\n",
    "plt.ylabel(\"x2\", fontsize = 14)\n",
    "plt.xlim(-1, 1)\n",
    "plt.ylim(-1, 1)\n",
    "plt.grid(alpha=0.2)\n",
    "plt.legend(handles=scatter.legend_elements()[0], labels=[\"Label 0\", \"Label 1\"])\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "E674wbDjxnEQ"
   },
   "source": [
    "Let's now consider two linear classifiers A and B defined by the coefficient vectors $\\boldsymbol{w}_A=[0, 1, 1]$ and $\\boldsymbol{w}_B=[0, 1, -1]$, respectively. Let's show their decision regions first:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yKVHEri8nYRZ"
   },
   "outputs": [],
   "source": [
    "# Coefficients of the classifier\n",
    "wA = np.array([0, 1, 1], ndmin=2).T\n",
    "wB = np.array([0, 1, -1], ndmin=2).T\n",
    "\n",
    "# 100x100 samples used to visualise the decision regions, design matrix and predicted labels for each sample\n",
    "coor1 = np.linspace(-2, 2, 100)\n",
    "coor2 = np.linspace(-2, 2, 100)\n",
    "x1, x2 = np.meshgrid(coor1, coor2)\n",
    "X = np.column_stack([np.ones(x1.ravel().shape), x1.ravel(), x2.ravel()])\n",
    "yPredA = prediction(X, wA)\n",
    "yPredB = prediction(X, wB)\n",
    "\n",
    "# Shows decision regiones by showing as an image the predicted label of the 100x100 samples\n",
    "image = plt.imshow(yPredA.reshape((100, 100)), \n",
    "                          cmap = 'bwr', \n",
    "                          interpolation='none', \n",
    "                          extent = (-2, 2, -2, 2), \n",
    "                          origin = 'lower', \n",
    "                          alpha = 0.9)\n",
    "\n",
    "plt.title(\"Classifier A\")\n",
    "plt.xlabel(\"x1\", fontsize=18)\n",
    "plt.ylabel(\"x2\", fontsize=18)\n",
    "plt.grid(True)\n",
    "plt.gca().set_aspect('equal', adjustable='box')\n",
    "plt.legend(handles=scatter.legend_elements()[0], labels=[\"Label 0\", \"Label 1\"])\n",
    "\n",
    "plt.figure()\n",
    "\n",
    "image = plt.imshow(yPredB.reshape((100, 100)), \n",
    "                          cmap = 'bwr', \n",
    "                          interpolation='none', \n",
    "                          extent = (-2, 2, -2, 2), \n",
    "                          origin = 'lower', \n",
    "                          alpha = 0.9)\n",
    "\n",
    "plt.title(\"Classifier B\")\n",
    "plt.xlabel(\"x1\", fontsize=18)\n",
    "plt.ylabel(\"x2\", fontsize=18)\n",
    "plt.grid(True)\n",
    "plt.gca().set_aspect('equal', adjustable='box')\n",
    "plt.legend(handles=scatter.legend_elements()[0], labels=[\"Label 0\", \"Label 1\"])\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3J59jI-g4HSm"
   },
   "source": [
    "Visually, which classifier do you think will do a better job? Let's obtain their training accuracies, error rate, likelihood and negative log-likelihood."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QL6aRLEz4WRS"
   },
   "outputs": [],
   "source": [
    "# Desing matrix, predicted label\n",
    "X_train = np.column_stack([np.ones(x1_train.shape), x1_train, x2_train])\n",
    "y_trainPredA = prediction(X_train, wA)\n",
    "y_trainPredB = prediction(X_train, wB)\n",
    "\n",
    "# Likelihood, neg log-likelihood, accuracy and error rate for classifier A\n",
    "LA = likelihood(X_train, wA, y_train)\n",
    "nlogLA = negLogLikelihood(X_train, wA, y_train)\n",
    "AA = accuracy(y_train, y_trainPredA)\n",
    "EA = 1 - AA\n",
    "\n",
    "# Likelihood, neg log-likelihood, accuracy and error rate for classifier B\n",
    "LB = likelihood(X_train, wB, y_train)\n",
    "nlogLB = negLogLikelihood(X_train, wB, y_train)\n",
    "AB = accuracy(y_train, y_trainPredB)\n",
    "EB = 1 - AB\n",
    "\n",
    "# Plots dataset and classifier A regions, prints likelihood, accuracy, etc\n",
    "plt.figure(figsize=(7, 7))\n",
    "image = plt.imshow(yPredA.reshape((100, 100)), \n",
    "                          cmap = 'bwr', \n",
    "                          interpolation='none', \n",
    "                          extent = (-1, 1, -1, 1), \n",
    "                          origin = 'lower', \n",
    "                          alpha = 0.2)\n",
    "scatter = plt.scatter(x1_train, x2_train, c=y_train, cmap=plt.get_cmap('bwr'))\n",
    "plt.title(\"Classifier A\")\n",
    "plt.xlabel(\"x1\", fontsize=18)\n",
    "plt.ylabel(\"x2\", fontsize=18)\n",
    "plt.grid(True)\n",
    "plt.gca().set_aspect('equal', adjustable='box')\n",
    "plt.legend(handles=scatter.legend_elements()[0], labels=[\"Label 0\", \"Label 1\"])\n",
    "\n",
    "\n",
    "print(\"    CLASSIFIER A\")\n",
    "print(\"The likelihood is L=\", LA)\n",
    "print(\"The negative log-likelihood is -log(L)=\",nlogLA)\n",
    "print(\"The accuracy is \", AA)\n",
    "print(\"The error rate is \", EA)\n",
    "\n",
    "\n",
    "# Plots dataset and classifier B regions, prints likelihood, accuracy, etc\n",
    "plt.figure(figsize=(7, 7))\n",
    "image = plt.imshow(yPredB.reshape((100, 100)), \n",
    "                          cmap = 'bwr', \n",
    "                          interpolation='none', \n",
    "                          extent = (-1, 1, -1, 1), \n",
    "                          origin = 'lower', \n",
    "                          alpha = 0.2)\n",
    "scatter = plt.scatter(x1_train, x2_train, c=y_train, cmap=plt.get_cmap('bwr'))\n",
    "plt.title(\"Classifier A\")\n",
    "plt.xlabel(\"x1\", fontsize=18)\n",
    "plt.ylabel(\"x2\", fontsize=18)\n",
    "plt.grid(True)\n",
    "plt.gca().set_aspect('equal', adjustable='box')\n",
    "plt.legend(handles=scatter.legend_elements()[0], labels=[\"Label 0\", \"Label 1\"])\n",
    "\n",
    "\n",
    "print(\"\\n    CLASSIFIER B\")\n",
    "print(\"The likelihood is L=\", LB)\n",
    "print(\"The negative log-likelihood is -log(L)=\",nlogLB)\n",
    "print(\"The accuracy is \", AB)\n",
    "print(\"The error rate is \", round(EB,2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PT-XqvXL-qTk"
   },
   "source": [
    "Based on these results, if we had to choose between the two, it is clear that we would select classifier A. It is also worth noting that the likelihood of both classifiers is a very small number. Remember that the likelihood is obtained by multiplying individual certainties, which are always less than one. Therefore, the larger the dataset, the closer the likelihood will be to zero. This can lead to **underflow**, which is a situation where your computer cannot represent a very small number and it is one of the reasons why we use the log-likelihood, as its values will not suffer from underflow.\n",
    "\n",
    "The following cell represents the quantity $p(\\boldsymbol{x}^T\\boldsymbol{w})$ in every point of the predictor space for a linear boundary defined by the coefficients `wTryMe` Essentially, you will be looking at the logistic function in 2D. Go ahead and change the value of the coefficients to see what happens with those values. For instance: \n",
    "- Multiply all the coefficents by the same positive number, what do you observe? Does the accuracty change? And the likelihood? \n",
    "- What if you multiply all the coefficients by -1? \n",
    "- Change the coefficients individually to see what happens to the linear boundary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WXdtCpOr4JLn"
   },
   "outputs": [],
   "source": [
    "# Change the coefficients below to see how the classifier changes\n",
    "wTryMe = np.array([0, 1, 1], ndmin=2).T\n",
    "\n",
    "# 100x100 samples used to visualise the decision regions, design matrix and predicted labels for each sample\n",
    "coor1 = np.linspace(-2, 2, 100)\n",
    "coor2 = np.linspace(-2, 2, 100)\n",
    "x1, x2 = np.meshgrid(coor1, coor2)\n",
    "X = np.column_stack([np.ones(x1.ravel().shape), x1.ravel(), x2.ravel()])\n",
    "\n",
    "# Plots dataset and classifier TryMe regions, prints likelihood, accuracy, etc\n",
    "plt.figure(figsize=(7, 7))\n",
    "image = plt.imshow(p(X, wTryMe).reshape((100, 100)), \n",
    "                          cmap = 'jet', \n",
    "                          interpolation='none', \n",
    "                          extent = (-1, 1, -1, 1), \n",
    "                          origin = 'lower', \n",
    "                          alpha = 0.9)\n",
    "plt.colorbar()\n",
    "\n",
    "scatter = plt.scatter(x1_train, x2_train, c=y_train, cmap=plt.get_cmap('bwr'))\n",
    "plt.title(\"Classifier TryMe\")\n",
    "plt.xlabel(\"x1\", fontsize=18)\n",
    "plt.ylabel(\"x2\", fontsize=18)\n",
    "plt.grid(True)\n",
    "plt.gca().set_aspect('equal', adjustable='box')\n",
    "plt.legend(handles=scatter.legend_elements()[0], labels=[\"Label 0\", \"Label 1\"])\n",
    "\n",
    "plt.show()\n",
    "\n",
    "y_trainTryMe = prediction(X_train, wTryMe)\n",
    "\n",
    "# Likelihood, neg log-likelihood, accuracy and error rate for classifier A\n",
    "LTryMe = likelihood(X_train, wTryMe, y_train)\n",
    "nlogLTryMe = negLogLikelihood(X_train, wTryMe, y_train)\n",
    "ATryMe = accuracy(y_train, y_trainTryMe)\n",
    "ETryMe = 1 - ATryMe\n",
    "\n",
    "print(\"    CLASSIFIER TRYME\")\n",
    "print(\"The likelihood is L=\", LTryMe)\n",
    "print(\"The negative log-likelihood is -log(L)=\",nlogLTryMe)\n",
    "print(\"The accuracy is \", ATryMe)\n",
    "print(\"The error rate is \", ETryMe)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FhnU3dD6vu-6"
   },
   "source": [
    "# Logistic regression: Optimisation\n",
    "\n",
    "We have used the logistic function to create a notion of certainty for classifiers, namely the likelihood (or equivalently, the negative log-likelihood). We have seen that classifiers that separate well samples from different classes have a high likelihood, whereas classifiers that produce a poor separation have low likelihood values.\n",
    "\n",
    "**Logistic \"regression\" classifiers** use the likelihood function $L$ as the objective function. Given a dataset, the best classifier is defined as the one with the **highest likelihood** value. This classifier also provides the **highest log-likelihood** value and obviously the **lowest negative log-likelihood** value.\n",
    "\n",
    "A separate question is, how do we find the classifier with the highest likelihood? For this, we need to use an **optimisation** strategy. In this section we will implement two approaches to find the logistic regression solution, namely **exhaustive search** and **gradient descent**. We will use the error surface defined by **negative log-likelihood** to find the logistic regression solution. This surface is **convex**, so we don't need to worry about local minima. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "exJuGJGZ1xUx"
   },
   "source": [
    "**Exhaustive approaches** basically evaluate a large number of options individually and select the model with the best performance. In the following cell we evaluate 8000 different coefficient vectors and select the one with the lowest negative log-likelihood as our solution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_CK3MKCipqo4"
   },
   "outputs": [],
   "source": [
    "from itertools import product\n",
    "\n",
    "lw0 = np.linspace(-1, 1, 20)\n",
    "lw1 = np.linspace(-1, 1, 20)\n",
    "lw2 = np.linspace(-1, 1, 20)\n",
    "\n",
    "all_possible_weights = product(lw0, lw1, lw2)\n",
    "lowest_negLogLike = np.Infinity\n",
    "\n",
    "for i, weights in enumerate(all_possible_weights):\n",
    "  current_negLogLike = negLogLikelihood(X_train, np.array(weights, ndmin=2).T, y_train)\n",
    "  if current_negLogLike < lowest_negLogLike:\n",
    "    best_weights = np.array(weights, ndmin=2).T\n",
    "    lowest_negLogLike = current_negLogLike\n",
    "    best_y_train_pred = prediction(X_train, best_weights)\n",
    "    print('\\n \\n New best w = {} | '.format(best_weights.ravel()), end='')\n",
    "    print('-Log(L) = {:8.5f} | '.format(current_negLogLike), end='')\n",
    "    print('Accuracy = {:.2f}'.format(accuracy(y_train, best_y_train_pred)), end='')\n",
    "    \n",
    "\n",
    "print('\\n \\n We have explored', i+1, 'different coefficient vectors and the best one is:', best_weights.ravel())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cgyjeCvpC3bJ"
   },
   "source": [
    "Exhaustive approaches are computationally inefficient. **Gradient descent** appoaches follow the slope of the error surface to reach the optimal solution and require evaluating fewer models. We will try gradient descent next.\n",
    "\n",
    "Instead of implementing the gradient descent algorithms ourselves, we are going to use the `minimize` function from the `optimization` module implemented as part of the `scipy` library. It takes a loss (or error) function, its gradient, and an initial solution along with some other parameters and outputs the final solution. To implement gradient descent we need the gradient of the error surface, which is provided in the following cell: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oTMduxhODGv1"
   },
   "outputs": [],
   "source": [
    "def gradNegLogLikelihood(X, w, y):\n",
    "  return -np.dot(X.T, (y - expit(np.dot(X, w)).reshape(y.shape))).ravel()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KT_o289DDNaD"
   },
   "source": [
    "We can derive this gradient from the mathematical expression for the negative log-likelihood, however let's not worry about it, you can find it in many textbooks and online, if you are curious about it. In the next cell, we use gradient descent to find the logistic regression solution: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bY6qiweywIBp"
   },
   "outputs": [],
   "source": [
    "import scipy.optimize\n",
    "np.set_printoptions(formatter={'float': '{: 0.10f}'.format})\n",
    "\n",
    "# Initial guess\n",
    "initial_w = np.array([1, -1, 1])\n",
    "\n",
    "# The loss function to be minimized (i.e. neg log-likelihood)\n",
    "def error_func(w):  \n",
    "  return negLogLikelihood(X_train, w, y_train.ravel())\n",
    "\n",
    "# The gradient of loss function\n",
    "def grad_func(w):\n",
    "  return gradNegLogLikelihood(X_train, w, y_train)\n",
    "\n",
    "# Store the path that the optimisation takes to get to the optimal solution\n",
    "def make_minimize_cb(path=[]):\n",
    "    \n",
    "    def minimize_cb(xk):\n",
    "        # note that we make a deep copy of xk\n",
    "        path.append(np.copy(xk))\n",
    "\n",
    "    return minimize_cb\n",
    "  \n",
    "path_ = [initial_w]\n",
    "\n",
    "# Here is where we run gradient descent\n",
    "res = scipy.optimize.minimize(fun = error_func, x0 = initial_w, \n",
    "                              jac = grad_func, tol=1e-6, \n",
    "                              callback=make_minimize_cb(path_))\n",
    "\n",
    "path = np.array(path_)[:,1:].T\n",
    "solution_w = res.x.reshape(3,1)\n",
    "negL = negLogLikelihood(X_train, solution_w, y_train)\n",
    "gradientL = gradNegLogLikelihood(X_train, solution_w, y_train)\n",
    "\n",
    "\n",
    "# Here we test our solution using the test dataset\n",
    "x1_test = df_test['Attribute_1'].to_numpy().reshape(N,1)\n",
    "x2_test = df_test['Attribute_2'].to_numpy().reshape(N,1)\n",
    "X_test = np.column_stack([np.ones(x1_test.shape), x1_test, x2_test])\n",
    "y_test = df_test['Label'].to_numpy().reshape(N,1)\n",
    "\n",
    "\n",
    "print('Computed optimal weights:\\t\\t {}'.format(res.x))\n",
    "print('Optimal loss (negative-log-likelihood):\\t {:.4f}'.format(negL))\n",
    "print('Gradient at the optimum point:\\t\\t {}'.format(gradientL))\n",
    "print('Train accuracy:\\t\\t\\t\\t {:.2f}'.format(accuracy(y_train, prediction(X_train, solution_w).reshape(101,1))))\n",
    "print('Test accuracy:\\t\\t\\t\\t {:.2f}'.format(accuracy(y_test, prediction(X_test, solution_w).reshape(101,1))))\n",
    "print('\\n\\nAnd here are all the intermediate solutions the algorithm visited to get to the optimal solution:\\n')\n",
    "for w in path_:\n",
    "  print('[w_0, w_1, w_2] = {}'.format(w))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KOslA78C3-Uy"
   },
   "source": [
    "In addition to the final solution, we have stored all the intermidiate solutions that we have visited before returning the final solution. As you can see, we have evaluated less than 20 models. \n",
    "\n",
    "Gradient descent is best understood by looking at the **empirical error surface**. A linear classifier in a 2D predictor space is defined by 3 parameters, and hence it is not possible to visualise the error surface, as we would need a 4D representation (3 parameters + 1 error). \n",
    "\n",
    "To illustrate gradient descent, we are going to consider the problem of finding the optimal values for the coefficients w1 and w2, and we will set w0 to the optimal value found in the previous section. We now have an optimisation problem where we have 2 parameters to tune, and hence we can visualise the error surface.\n",
    "\n",
    "The following cell plots the error surface:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NV2RKLfKbE-o"
   },
   "outputs": [],
   "source": [
    "from mpl_toolkits import mplot3d\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from matplotlib.colors import LogNorm\n",
    "%matplotlib inline\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "def error_func_2(w1, w2):\n",
    "  return error_func(np.array([res.x[0], w1, w2]))\n",
    "\n",
    "np_error = np.frompyfunc(error_func_2, 2, 1)\n",
    "\n",
    "minima_ = np.array([res.x[1],  res.x[2]])\n",
    "\n",
    "# Defines a grid\n",
    "xmin, xmax, xstep = -2.5, 12.5, .2\n",
    "ymin, ymax, ystep = -2.5, 12.5, .2\n",
    "x, y = np.meshgrid(np.arange(xmin, xmax + xstep, xstep), np.arange(ymin, ymax + ystep, ystep))\n",
    "z = np_error(x, y).astype(float)\n",
    "\n",
    "# Plots the error surface\n",
    "fig1 = plt.figure(figsize=(8, 5))\n",
    "ax1 = plt.axes(projection='3d', elev=50, azim=-50)\n",
    "ax1.plot_surface(x, y, z, norm=LogNorm(), rstride=1, cstride=1, \n",
    "                edgecolor='none', alpha=.7, cmap=plt.cm.jet)\n",
    "ax1.scatter3D(*minima_, error_func_2(*minima_), marker='*', color='r', s=150)\n",
    "ax1.set_xlabel('$w_1$', fontsize=14)\n",
    "ax1.set_ylabel('$w_2$', fontsize=14)\n",
    "ax1.set_zlabel('L',)\n",
    "ax1.set_xlim((xmin, xmax))\n",
    "ax1.set_ylim((ymin, ymax))\n",
    "\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LGvKuHyYN5lx"
   },
   "source": [
    "The optimal model is identified by a start (*) and corresponds to the model with the lowest error. \n",
    "\n",
    "In the next cell, we will obtain the sequence of solutions that gradient descent visits until finding the optimal model. In this case, the error surface is represented by contour lines:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2ZPd2hPvLLJ8"
   },
   "outputs": [],
   "source": [
    "from matplotlib import animation\n",
    "from IPython.display import HTML\n",
    "\n",
    "initial_weights = np.array([0, -1, 1])\n",
    "\n",
    "\n",
    "path_ = [initial_weights]\n",
    "\n",
    "# Implements gradient descent\n",
    "res = scipy.optimize.minimize(fun = error_func, x0 = initial_weights, \n",
    "                              jac = grad_func, tol=1e-6, callback=make_minimize_cb(path_))\n",
    "path = np.array(path_)[:,1:].T\n",
    "\n",
    "# Plots the trajectory in the solution space until the optimal solution has been reached\n",
    "fig2, ax2 = plt.subplots(figsize=(10, 6))\n",
    "ax2.contour(x, y, z, levels=np.logspace(0, 3, 50), norm=LogNorm(), cmap=plt.cm.jet)\n",
    "ax2.quiver(path[0,:-1], path[1,:-1], path[0,1:]-path[0,:-1], path[1,1:]-path[1,:-1], scale_units='xy', angles='xy', scale=1, color='k')\n",
    "ax2.plot(*minima_, 'r*', markersize=18)\n",
    "ax2.set_xlabel('$w_1$')\n",
    "ax2.set_ylabel('$w_2$')\n",
    "ax2.set_xlim((xmin, xmax))\n",
    "ax2.set_ylim((ymin, ymax))\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "87UCZMRGwSMR"
   },
   "source": [
    "# Summary\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4lI96cT1PVd1"
   },
   "source": [
    "In this lab we have explored linear classifiers. Classifiers can be seen as partitions of the predictor space into **decision regions** separated **decision boundaries**. Each decision region is associated to one single label so that samples that lie within the same decision region are assigned the same label.\n",
    "\n",
    "We have considered linear classifiers in 2D predictor spaces, i.e. classification problems with two predictors. Low-dimensional predictor spaces can be visualised and are very useful to develop our intuition. This intuition will be crucial to understand high-dimenional problems that cannot be visualised. Note that the **mathematical notation for low-dimentional and high-dimensional preditor spaces is identical** and won't require any additional effort. Symbols $\\boldsymbol{x}$ and $\\boldsymbol{w}$ can be used to represent preditor spaces of any dimensions, 1D, 2D, 3D... 10D, 100D. That's why vector notation is so powerful and you should get comfortable with it. At the end of the day, it's all about **understanding what each operation on vectors means, not carrying out the actual calculation**. Computers do the calculations, but understanding is your responsibility. For instance, you should know that $\\boldsymbol{x}^T\\boldsymbol{w}$ is the distance between a sample $\\boldsymbol{x}$ and a linear boundary defined by $\\boldsymbol{w}$. This boundary might be a hypersurface in a 55D predictor space, $\\boldsymbol{x}^T\\boldsymbol{w}$ will still be a distance.\n",
    "\n",
    "In addition to classifiers, we have also touched upon **optimisation**. You should understand the role of optimisation in machine learning and know the basic optimisation strategies, such as **gradient descent**. Most optimisation strategies will have been already implemented in your future machine learning framework, so your job will be to choose the most suitable one for your problem. There exist many variations of gradient descent, so in the future you will need to have an open mind and learn about optimisation approaches that you might not have heard of. \n",
    "\n",
    "Remember to play with the notebook. The more you do it, the more intuition you wil develop. In the past we had to rely on mental experimentation to understand maths. Nowadays, we can use computers to take this experimentation further.\n",
    "\n",
    "If you feel that you have a good understanding of the concepts that we have covered, go ahead and attempt the corresponding quiz. After attempting the quiz, you can come back to this notebook to experiment a little bit more.\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "ECS7020P_Lab05.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
