{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "2a62f054",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import my_utils as mu\n",
    "import pandas as pd\n",
    "import torch.utils.data as data\n",
    "from torch import nn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb6d8b1f",
   "metadata": {},
   "source": [
    "# Elliot Linsey Neural Network Exam ECS659P June 2022"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7e9114e",
   "metadata": {},
   "source": [
    "## Q1.A\n",
    "\n",
    "We use models with more than one hidden layer in order to extract more features from our input data and model more complex interactions. Also, we can introduce non-linear relationships using activation functions that are more accurately able to model data, an example being the relu function which can be seen in Multi-Layer Perceptrons (MLPs). In making a model deeper, we learn high-level features from our data in an incremental manner, whereas with a shallow network we may only recognise lines and edges, with a deep network we can recognise concepts such as 'face' or 'dog'. \n",
    "Deeper neural networks are also better approximators, it is far easier to learn functions using deeper networks than with shallow ones. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7261f84",
   "metadata": {},
   "source": [
    "## Q1.B\n",
    "\n",
    "One technique used is batch normalisation. This normalises per channel by subtracting the mean and dividing by the standard deviation to give us a mean of 0 and standard deviation of 1. This is then rescaled channelwise using the parameter gamma, and an offset beta is also added. Both these parameters can be learned and optimised during the training process. What batch normalisation provides is extra stability to the neural network, allowing it to be trained longer as well as with a higher learning rate. It also makes it less sensitive to weight initialisation methods. \n",
    "\n",
    "Another technique is the skip connection seen in resnet blocks. This is where you add the input feature to the output of your block, so y = F(x) + x. This allows a direct path for propagating gradients during backpropagation. As the model gets deeper, it becomes less able to learn features, skip connections allow the output of one layer to skip ahead and be fed as input into the next layer to prevent this.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "929709f6",
   "metadata": {},
   "source": [
    "## Q2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "1e9d9222",
   "metadata": {},
   "outputs": [],
   "source": [
    "inp2 = torch.rand(16,4,128,128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "5673e795",
   "metadata": {},
   "outputs": [],
   "source": [
    "conv1 = nn.Conv2d(4,256,5,stride=1,padding=0)\n",
    "conv2 = nn.Conv2d(64,64,3,stride=1,padding=0)\n",
    "fl = nn.Flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "7f47d444",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([16, 256, 124, 124])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ex1 = conv1(inp2)\n",
    "ex1.size()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74290442",
   "metadata": {},
   "source": [
    "weights = 256x4x5x5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "13ed122c",
   "metadata": {},
   "outputs": [],
   "source": [
    "inp3 = torch.rand(16,64,124,124)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "15f10035",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([16, 64, 122, 122])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ex2 = conv2(inp3)\n",
    "ex2.size()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b893cec7",
   "metadata": {},
   "source": [
    "weights = 4x(64x64x3x3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "982ffbaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "inp4 = torch.rand(16,256,122,122)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "8e7caf03",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([16, 3810304])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ex3 = fl(inp4)\n",
    "ex3.size()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66427bb3",
   "metadata": {},
   "source": [
    "weights = 3810304 x 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "80062e54",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "weights with no avg pooling 76379136\n"
     ]
    }
   ],
   "source": [
    "print('weights with no avg pooling ' + str((256*4*5*5)+(4*(64*64*3*3))+(3810304*20)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "df4acfb3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "weights with avg pooling 178176\n"
     ]
    }
   ],
   "source": [
    "print('weights with avg pooling ' + str((256*4*5*5)+(4*(64*64*3*3))+(256*20)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d79690b1",
   "metadata": {},
   "source": [
    "The first weights tensor is [256x4x5x5]\n",
    "\n",
    "The second weights tensor is 4x[64x64x3x3]\n",
    "\n",
    "This leaves us with dimensions of [256,122,122] when reconstituted\n",
    "\n",
    "Flattened to a linear classifier this becomes [3810304,20]\n",
    "\n",
    "weights = (256x4x5x5)+(4x(64x64x3x3))+(3810304x20) = 76379136\n",
    "\n",
    "The question mentions an avg pooling layer which is not used. I'm assuming that if this was used prior to flattening we would instead result with:\n",
    "\n",
    "[256,1,1] after being reconstituted and pooled\n",
    "\n",
    "This results in a linear classifier of [256,20]\n",
    "\n",
    "The weights then equal (256x4x5x5)+(4x(64x64x3x3))+(256x20) = 178176 "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e55e2ad",
   "metadata": {},
   "source": [
    "## Q3\n",
    "\n",
    "This appears to be a type of CNN with a skip connection, due to the expand_channels with a 1x1 CNN layer. However, there are some issues with this. Firstly, identity appears to be taking the place of 'x', however he first applies the stem function to 'x' before assigning 'identity' to it. In a skip connection, identity should be untouched and defined as 'identity = x' before the stem function. \n",
    "\n",
    "Also, the order of batch normalisation, relu and convolution is not optimal. A resnet block consists of convolution followed by batch normalisation followed by relu. \n",
    "\n",
    "After this he defines 'x = identity', in a skip connection it should be 'x = x + identity', otherwise all the calculations you have done are discarded when x becomes identity. \n",
    "\n",
    "After this he follows it by a relu then the last convolution, it should be the other way around with convolution followed by relu or perhaps remove the last convolutional layer."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f92014c8",
   "metadata": {},
   "source": [
    "## Q4.A\n",
    "\n",
    "The VGG net utilises a convolution arch to determine the number of convolutional layers and the number of output channels. The creators of VGG found that narrow convolutions of size 3x3 worked optimally, therefore I will set the kernel sizes to this, although later this can be changed and evaluated. Due to the large number of classes, it is most likely best to use deep and narrow convolutional kernel sizes. Then, utilising the convolutional arch I would run tests starting with only 1 convolutional layer to limit the number of parameters. I could see how high an accuracy I could achieve by using N number of VGG blocks using only one convolutional layer and increasing the number of output channels, then I would investigate the difference in accuracy when I start to add another convolutional layer to each VGG block, one at a time. I could also experiment with max pooling kernels and stride to reduce the feature maps even further.\n",
    "\n",
    "When there is a number of N blocks I have identified that achieves 90% accuracy, it could be useful to utilise a gridsearch mechanism to further investigate parameters such as learning rate, convolutional kernels and stride/padding, as well as different optimisers like SGD or Adam. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1183cba",
   "metadata": {},
   "source": [
    "## Q4.B\n",
    "\n",
    "A traditional RNN has a form of short-term memory and is not very good at remembering long-term information due to vanishing gradients. A version that allows greater long-term memory is that of Long Short Term Memory or LSTM. This adds a forget gate, input gate, output gate and memory cell to the RNN. \n",
    "\n",
    "The forget gate determines how much of the previous memory cell information is contained, for example if the previous information stored in the memory cell was 'Wednesday', but we encounter a word further along of 'Thursday', the forget gate will determine how much of the original 'Wednesday' information in the memory cell should be forgotten. \n",
    "\n",
    "The input gate determines how much information is taken into the candidate memory cell. It essentially determines how important a word is and whether it should be remembered or not, for example it will determine how much of the current input 'Thursday' should be remembered. \n",
    "\n",
    "The candidate memory cell is a result of applying a tanh activation function to a matrix multiplication of the input matrix and weights plus the previous hidden state matrix and weights. \n",
    "\n",
    "When put together, the memory cell $C_t$ is the result of the forget gate elementwise multiplying the previous memory cell plus the input gate elementwise multiplying the candidate memory cell. Therefore, if the forget gate has low values and the input gate has high values, the memory cell will be updated with the new candidate memory cell information. \n",
    "\n",
    "Both the input gate and forget gate are fully-connected layers with activation functions (such as sigmoid or tanh). "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
