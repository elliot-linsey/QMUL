{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tRYKVntud1eW",
    "origin_pos": 0,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Optimization\n",
    "\n",
    "* In Deep Learning, we *train* models, updating them successively so that they get better and better as they see more and more training data.\n",
    "   * *Optimization*: the process of fitting our models to observed data\n",
    "   * To understand optimization problems and methods, we need to know basic concepts from *Calculus*.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Recap from last week\n",
    "\n",
    "* Estimate blood pressure from height, weight, age, exerc_hours"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "# input_data X: [height, weight, age, exerc_hours]\n",
    "X = [[180, 89, 35, 1],\n",
    "     [160, 49, 40, 4],\n",
    "     [179, 69, 20, 2]];\n",
    "\n",
    "# ground truth values Y: blood_pressure\n",
    "Y = [[160], [130], [110]];"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* If the model parameters are known:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "121.2\n"
     ]
    }
   ],
   "source": [
    "# model w: w = [w0, w1, w2, w3]\n",
    "w = [0.3, 0.8, -0.4, 10];\n",
    "\n",
    "def model(w, x):\n",
    "    y_hat = w[0]*x[0]+w[1]*x[1]+w[2]*x[2]+w[3]*x[3]\n",
    "    return y_hat\n",
    "\n",
    "x0 = X[0]\n",
    "y0_hat = model(w, x0)\n",
    "print(y0_hat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "* If the model parameters are *unknown*, we can use ground truth labels to find them\n",
    "\n",
    "$$loss= (y-\\hat{y})^2$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1505.4399999999998\n"
     ]
    }
   ],
   "source": [
    "def loss(y, y_hat):\n",
    "    return (y-y_hat)**2\n",
    "\n",
    "y0 = Y[0][0]\n",
    "print(loss(y0, y0_hat))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "* The update for the first parameter $w[0]$ is given by:\n",
    "$$w[0] \\leftarrow w[0] - \\frac{\\theta loss}{\\theta w[0]}$$\n",
    "* In this lecture, we will see what $\\frac{\\theta loss}{\\theta w[0]}$ is all about.\n",
    "* For that, we need to know basic concepts from *Calculus*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Functions\n",
    "* In maths, functions are similar to functions in programming\n",
    "* Given a set of input values, a function can be used to compute an output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Python:\n",
    "def f(w, x):\n",
    "    y = w[0]*x[0]+w[1]*x[1]+w[2]*x[2]+w[3]*x[3]\n",
    "    return y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Maths: $$ y = f(w_0, w_1, w_2, w_3, x_0, x_1, x_2, x_3)\\\\ y = w_0 \\cdot x_0 + w_1 \\cdot x_1 + w_2 \\cdot x_2 + w_3 \\cdot x_3$$\n",
    "* The above function has 8 input variables. \n",
    "* Let's assume for simplicity a single variable function: $y=f(x) \\textrm{ where } f: \\mathbb{R} \\rightarrow \\mathbb{R}$.\n",
    "* Example: Define the function $f(x) = 3x^2-4x$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import torch \n",
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "def f(x):\n",
    "    return 3 * x ** 2 - 4 * x\n",
    "\n",
    "x = torch.arange(-5, 5, 0.1)\n",
    "y = f(x)\n",
    "plt.plot(x,y);\n",
    "plt.xlabel('x'); plt.ylabel('y=f(x)'); "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Derivatives and Differentiation\n",
    "\n",
    "* Suppose that we have a function $f: \\mathbb{R} \\rightarrow \\mathbb{R}$, whose input and output are both scalars. The *derivative* of $f$ is defined as:\n",
    "$$f'(x) = \\lim_{h \\rightarrow 0} \\frac{f(x+h) - f(x)}{h},$$\n",
    "if this limit exists. \n",
    "\n",
    "* We can interpret the derivative $f'(x)$ as the *instantaneous* rate of change of $f(x)$ with respect to $x$.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LiQigeHpd1eX",
    "origin_pos": 4,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "* Example: Consider again the function $f(x) = 3x^2-4x$. For $x=1$ and by letting $h$ approach $0$, the derivative $f'(x)$ is getting equal to $2$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 510,
     "status": "ok",
     "timestamp": 1610905942183,
     "user": {
      "displayName": "Georgios Tzimiropoulos",
      "photoUrl": "",
      "userId": "03131831684678803743"
     },
     "user_tz": 0
    },
    "id": "t68EKfmad1eY",
    "origin_pos": 5,
    "outputId": "520c62d8-0ad3-4200-c426-9eb5705138a2",
    "scrolled": false,
    "tab": [
     "pytorch"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "h=0.10000, numerical limit=2.30000\n",
      "h=0.01000, numerical limit=2.03000\n",
      "h=0.00100, numerical limit=2.00300\n",
      "h=0.00010, numerical limit=2.00030\n",
      "h=0.00001, numerical limit=2.00003\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "def f(x):\n",
    "    return 3 * x ** 2 - 4 * x\n",
    "\n",
    "def numerical_lim(f, x, h):\n",
    "    return (f(x + h) - f(x)) / h\n",
    "\n",
    "h = 0.1\n",
    "for i in range(5):\n",
    "    print(f'h={h:.5f}, numerical limit={numerical_lim(f, 1, h):.5f}')\n",
    "    h *= 0.1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "* If $f$ is differentiable at every number of an interval, then this function is differentiable on this interval.\n",
    "* This also means that the derivative is also a function.\n",
    "* For example the derivative of $f(x) = 3x^2-4x$ is also function $f'(x) = 6x-4$.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "def f_prime(x):\n",
    "    return 6 * x - 4\n",
    "\n",
    "x = torch.arange(-5, 5, 0.1)\n",
    "plt.plot(x,f_prime(x));\n",
    "plt.xlabel('x'); plt.ylabel(\"y=f'(x)\"); "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4GqBiCjbd1eZ",
    "origin_pos": 6,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Differentiation rules\n",
    "\n",
    "* Given $y = f(x)$, the following expressions are equivalent:\n",
    "\n",
    "$$f'(x) = y' = \\frac{dy}{dx} = \\frac{df}{dx} = \\frac{d}{dx} f(x) = Df(x) = D_x f(x),$$\n",
    "\n",
    "where symbols $\\frac{d}{dx}$ and $D$ are *differentiation operators* that indicate operation of *differentiation*.\n",
    "\n",
    "* Differentiation results of commonly-used functions:\n",
    "    * $DC = 0$ ($C$ is a constant),\n",
    "    * $Dx^n = nx^{n-1}$ (the *power rule*, $n$ is any real number),\n",
    "    * $De^x = e^x$,\n",
    "    * $D\\ln(x) = 1/x.$\n",
    "    \n",
    "* The above results are just examples: there are many more functions for which the derivatives are readily available.     "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JqZOQKxxd1ea",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "* Suppose that functions $f$ and $g$ are both differentiable and $C$ is a constant,\n",
    "we have: \n",
    "    * the *constant multiple rule*\n",
    "    \n",
    "    $$\\frac{d}{dx} [Cf(x)] = C \\frac{d}{dx} f(x),$$\n",
    "    \n",
    "    * the *sum rule*\n",
    "\n",
    "    $$\\frac{d}{dx} [f(x) + g(x)] = \\frac{d}{dx} f(x) + \\frac{d}{dx} g(x),$$\n",
    "\n",
    "    * the *product rule*\n",
    "\n",
    "    $$\\frac{d}{dx} [f(x)g(x)] = f(x) \\frac{d}{dx} [g(x)] + g(x) \\frac{d}{dx} [f(x)],$$\n",
    "\n",
    "    * and the *quotient rule*\n",
    "\n",
    "$$\\frac{d}{dx} \\left[\\frac{f(x)}{g(x)}\\right] = \\frac{g(x) \\frac{d}{dx} [f(x)] - f(x) \\frac{d}{dx} [g(x)]}{[g(x)]^2}.$$\n",
    "\n",
    "* Now we can apply a few of the above rules to find $f'(x) = 3 \\frac{d}{dx} x^2-4\\frac{d}{dx}x = 6x-4$. Thus, by setting $x = 1$, we have $f'(1) = 2$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_Hkzv8g9d1ea",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Visualization\n",
    "\n",
    "* This derivative is also the slope of the tangent line to the curve $f(x)$ when $x = 1$.\n",
    "\n",
    "![derivative as slope of tangent line.](img/derivative_example3.png) \n",
    "\n",
    "<!-- ![derivative as slope of tangent line.](https://drive.google.com/uc?export=view&id=1QwpZpy-SGuDAWPF_uvpq7S5GRjZ-BmEc) -->\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5ebTpBZ0d1eb",
    "origin_pos": 16,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Partial Derivatives\n",
    "\n",
    "* So far we have dealt with the differentiation of functions of just one variable. \n",
    "* In deep learning, functions often depend on *many* variables. Thus, we need to extend the ideas of differentiation to these *multivariate* functions.\n",
    "\n",
    "* Let $y = f(x_1, x_2, \\ldots, x_n)$ be a function with $n$ variables. \n",
    "* Function f takes the $n$ variables $x_1, x_2, \\ldots, x_n$ and maps them to a single number, i.e. y is *scalar*. \n",
    "* Equivalently, we can write $y=f(\\mathbf{x})$ where $\\mathbf{x} = [x_1, x_2, \\ldots, x_n]^\\top$ is the $n$-dimensional vector containing the input variables. \n",
    "* Example from above: $$y = w_0 \\cdot x_0 + w_1 \\cdot x_1 + w_2 \\cdot x_2 + w_3 \\cdot x_3$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "* The *partial derivative* of $y$ with respect to its $i^\\mathrm{th}$  parameter $x_i$ is:\n",
    "\n",
    "$$ \\frac{\\partial y}{\\partial x_i} = \\lim_{h \\rightarrow 0} \\frac{f(x_1, \\ldots, x_{i-1}, x_i+h, x_{i+1}, \\ldots, x_n) - f(x_1, \\ldots, x_i, \\ldots, x_n)}{h}.$$\n",
    "\n",
    "* To calculate $\\frac{\\partial y}{\\partial x_i}$, we can simply treat $x_1, \\ldots, x_{i-1}, x_{i+1}, \\ldots, x_n$ as constants and calculate the derivative of $y$ with respect to $x_i$.\n",
    "\n",
    "* In terms of notation of partial derivatives, the following are equivalent:\n",
    "\n",
    "$$\\frac{\\partial y}{\\partial x_i} = \\frac{\\partial f}{\\partial x_i} = f_{x_i} = f_i = D_i f = D_{x_i} f.$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AzpXj4Wfd1eb",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Gradients\n",
    "\n",
    "* We can concatenate partial derivatives of a multivariate function $f(\\mathbf{x})$ with respect to all its input variables to obtain the *gradient* vector of the function.\n",
    "\n",
    "* The gradient of the function $f(\\mathbf{x})$ with respect to $\\mathbf{x}$ is a vector of $n$ partial derivatives:\n",
    "\n",
    "$$\\nabla_{\\mathbf{x}} f(\\mathbf{x}) = \\bigg[\\frac{\\partial f(\\mathbf{x})}{\\partial x_1}, \\frac{\\partial f(\\mathbf{x})}{\\partial x_2}, \\ldots, \\frac{\\partial f(\\mathbf{x})}{\\partial x_n}\\bigg]^\\top,$$\n",
    "where $\\nabla_{\\mathbf{x}} f(\\mathbf{x})$ is often replaced by $\\nabla f(\\mathbf{x})$ when there is no ambiguity.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oXkteXuZd1eb",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Differentiation results of commonly-used multivariate functions\n",
    "\n",
    "The following results are often used when differentiating multivariate functions:\n",
    "\n",
    "* $$\\nabla_{\\mathbf{x}} \\mathbf{A} \\mathbf{x} = \\mathbf{A}^\\top$$\n",
    "\n",
    "* $$\\nabla_{\\mathbf{x}} \\mathbf{x}^\\top \\mathbf{A}  = \\mathbf{A}$$\n",
    "\n",
    "* $$\\nabla_{\\mathbf{x}} \\mathbf{x}^\\top \\mathbf{A} \\mathbf{x}  = (\\mathbf{A} + \\mathbf{A}^\\top)\\mathbf{x}$$\n",
    "\n",
    "* $$\\nabla_{\\mathbf{x}} \\|\\mathbf{x} \\|^2 = \\nabla_{\\mathbf{x}} \\mathbf{x}^\\top \\mathbf{x} = 2\\mathbf{x}$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qd8idSvdd1ed",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Chain Rule\n",
    "\n",
    "* In Deep Learning, the multivariate functions used are often *composite*, so the above rules cannot be directly applied.\n",
    "* Fortunately, the *chain rule* enables the differentiatiation of composite functions.\n",
    "\n",
    "* Let us first consider functions of a single variable. Suppose that functions $y=f(u)$ and $u=g(x)$ are both differentiable, then the chain rule states that\n",
    "\n",
    "$$\\frac{dy}{dx} = \\frac{dy}{du} \\frac{du}{dx}.$$\n",
    "\n",
    "* Suppose now that the differentiable function $y$ has variables $u_1, u_2, \\ldots, u_m$, where each differentiable function $u_i$ has variables $x_1, x_2, \\ldots, x_n$. Note that $y$ is a function of $x_1, x_2, \\ldots, x_n$.\n",
    "\n",
    "* Then, the chain rule gives\n",
    "$$\\frac{dy}{dx_i} = \\frac{dy}{du_1} \\frac{du_1}{dx_i} + \\frac{dy}{du_2} \\frac{du_2}{dx_i} + \\cdots + \\frac{dy}{du_m} \\frac{du_m}{dx_i}$$\n",
    "for any $i = 1, 2, \\ldots, n$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QEQvOJ7qd1ed",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Automatic Differentiation\n",
    "\n",
    "* Differentiation is a crucial step in nearly all deep learning optimization algorithms.\n",
    "* While the calculations for taking these derivatives is doable, for complex models, working out the calculations by hand can be a pain (and often error-prone).\n",
    "\n",
    "* Deep learning frameworks overcome this problem by automatically calculating derivatives, by *automatic differentiation*.\n",
    "\n",
    "* Given a model the system builds a *computational graph*, tracking which data combined through which operations to produce the output.\n",
    "\n",
    "* Automatic differentiation enables the system to subsequently calculate gradients.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_NJv2iWkd1ee",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# A Simple Example\n",
    "\n",
    "* Differentiate function $y = 2\\mathbf{x}^{\\top}\\mathbf{x}$ with respect to the column vector $\\mathbf{x}$.\n",
    "* Note that, in practice, AD allows the calculation of the derivative for a specific value of the input variable.\n",
    "* It does not provide a general mathematical formula for the function derivative. \n",
    "    * Good news is that you never need the formula in practice.\n",
    "* For example consider the function $f(x) = 3x^2-4x$. We know that its derivative is $f'(x) = 6x-4$. AD will not give you access to this formula. It will give you access to values of $f'(x)$ for specific input values of $x$, e.g. $f'(1)=f'(x)$ for $x=1$, $f'(10)=f'(x)$ for $x=10$ etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 3289,
     "status": "ok",
     "timestamp": 1610905960035,
     "user": {
      "displayName": "Georgios Tzimiropoulos",
      "photoUrl": "",
      "userId": "03131831684678803743"
     },
     "user_tz": 0
    },
    "id": "0OgGS7_1d1ee",
    "outputId": "ca65bee8-6774-4691-a37d-7bcddfd87ef0",
    "scrolled": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0., 1., 2., 3.], requires_grad=True)\n",
      "None\n",
      "tensor(28., grad_fn=<MulBackward0>)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "# create a specific value of input x\n",
    "x = torch.arange(4.0)\n",
    "x.requires_grad_(True)\n",
    "print(x)\n",
    "print(x.grad)\n",
    "y = 2*torch.dot(x, x)\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OxpprzIwd1ef",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "* Next, we can automatically calculate the gradient of `y` with respect to each component of `x` by calling the function for gradient calculation and printing the gradient.\n",
    "* This function is called **backpropagation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 480,
     "status": "ok",
     "timestamp": 1610905964876,
     "user": {
      "displayName": "Georgios Tzimiropoulos",
      "photoUrl": "",
      "userId": "03131831684678803743"
     },
     "user_tz": 0
    },
    "id": "jPAnXJ-nd1eg",
    "outputId": "3e802f75-f18b-44b8-966d-97d1c86bb333"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0.,  4.,  8., 12.])"
      ]
     },
     "execution_count": 3,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.backward()\n",
    "x.grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bdRkU-S7d1eh"
   },
   "source": [
    "* The gradient of the function $y = 2\\mathbf{x}^{\\top}\\mathbf{x}$ with respect to $\\mathbf{x}$ should be $4\\mathbf{x}$.\n",
    "* Let us quickly verify that our desired gradient was calculated correctly.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 566,
     "status": "ok",
     "timestamp": 1610905967980,
     "user": {
      "displayName": "Georgios Tzimiropoulos",
      "photoUrl": "",
      "userId": "03131831684678803743"
     },
     "user_tz": 0
    },
    "id": "JEQKyOted1eh",
    "outputId": "671b9934-25a8-48e1-f860-9b8257aa300a"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([True, True, True, True])"
      ]
     },
     "execution_count": 4,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.grad == 4 * x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 638,
     "status": "ok",
     "timestamp": 1610905969355,
     "user": {
      "displayName": "Georgios Tzimiropoulos",
      "photoUrl": "",
      "userId": "03131831684678803743"
     },
     "user_tz": 0
    },
    "id": "q9eee2rSd1ei",
    "outputId": "56c039c6-e792-4d51-ce80-5c102042a094",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1., 1., 1., 1.])"
      ]
     },
     "execution_count": 5,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Another example y = h(x) = x1+x2+x3+x4\n",
    "# PyTorch accumulates the gradient in default, we need to clear the previous \n",
    "# values\n",
    "x.grad.zero_() \n",
    "y = x.sum()\n",
    "y.backward()\n",
    "x.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 358
    },
    "collapsed": true,
    "executionInfo": {
     "elapsed": 675,
     "status": "error",
     "timestamp": 1610905970920,
     "user": {
      "displayName": "Georgios Tzimiropoulos",
      "photoUrl": "",
      "userId": "03131831684678803743"
     },
     "user_tz": 0
    },
    "id": "_4nT-g0nd1ei",
    "outputId": "855a703d-4899-4b50-e897-8eb42f1446f3",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "ignored",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-109fe2573949>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# will throw an error\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m    219\u001b[0m                 \u001b[0mretain_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    220\u001b[0m                 create_graph=create_graph)\n\u001b[0;32m--> 221\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    222\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    223\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m    124\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    125\u001b[0m     \u001b[0mgrad_tensors_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_tensor_or_tensors_to_tuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrad_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 126\u001b[0;31m     \u001b[0mgrad_tensors_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_make_grads\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    127\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mretain_graph\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    128\u001b[0m         \u001b[0mretain_graph\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36m_make_grads\u001b[0;34m(outputs, grads)\u001b[0m\n\u001b[1;32m     48\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequires_grad\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 50\u001b[0;31m                     \u001b[0;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"grad can be implicitly created only for scalar outputs\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     51\u001b[0m                 \u001b[0mnew_grads\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mones_like\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmemory_format\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpreserve_format\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: grad can be implicitly created only for scalar outputs"
     ]
    }
   ],
   "source": [
    "# output y must be always scalar\n",
    "x.grad.zero_() \n",
    "y = x * x\n",
    "y.backward() # will throw an error\n",
    "x.grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KgINuxezd1ei",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Detaching Computation\n",
    "\n",
    "* Sometimes, we wish to move some calculations outside of the recorded computational graph.\n",
    "* Example: consider $f(x) = 2\\mathbf{x}^{\\top}\\mathbf{x}$ and then function $g(x)=(f(x)-\\alpha)^2$ where $\\alpha$ is a constant. Let's asssume $\\alpha=2$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 519,
     "status": "ok",
     "timestamp": 1610905975983,
     "user": {
      "displayName": "Georgios Tzimiropoulos",
      "photoUrl": "",
      "userId": "03131831684678803743"
     },
     "user_tz": 0
    },
    "id": "d3hl6rk-d1ej",
    "outputId": "adfa76a3-edb7-4289-e284-574a5b03f1b5",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([  0., 208., 416., 624.])\n"
     ]
    }
   ],
   "source": [
    "# create a specific value of input x\n",
    "x = torch.arange(4.0)\n",
    "x.requires_grad_(True)\n",
    "f = 2*torch.dot(x, x)\n",
    "a = 2\n",
    "g = (f-a)**2\n",
    "g.backward()\n",
    "print(x.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QFDP9JJEd1ej",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "* Now consider the case where $\\alpha = x_1+ x_2 + x_3 + x_4$\n",
    "* If we want $\\alpha$ to be treated as constant i.e. $\\alpha=0+1+2+3=6$ then we must *detach* $\\alpha$ from the graph!\n",
    "* Otherwise it will be considered a function of input $\\mathbf{x}$!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 492,
     "status": "ok",
     "timestamp": 1610905978729,
     "user": {
      "displayName": "Georgios Tzimiropoulos",
      "photoUrl": "",
      "userId": "03131831684678803743"
     },
     "user_tz": 0
    },
    "id": "rI-HUS4Xd1ej",
    "outputId": "43dc34cb-d8fa-4b32-a268-5cd02155c631"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([  0., 176., 352., 528.])\n"
     ]
    }
   ],
   "source": [
    "x = torch.arange(4.0)\n",
    "x.requires_grad_(True)\n",
    "f = 2*torch.dot(x, x)\n",
    "a = x.sum().detach() #equivalent to a = 6\n",
    "#a = x.sum() # here a is a function of x\n",
    "g = (f-a)**2\n",
    "g.backward()\n",
    "print(x.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-44., 132., 308., 484.])\n"
     ]
    }
   ],
   "source": [
    "x = torch.arange(4.0)\n",
    "x.requires_grad_(True)\n",
    "f = 2*torch.dot(x, x)\n",
    "#a = x.sum().detach() #equivalent to a = 6\n",
    "a = x.sum() # here a is a function of x\n",
    "g = (f-a)**2\n",
    "g.backward()\n",
    "print(x.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# De-activating the autograd engine\n",
    "\n",
    "* If you want to avoid calculating gradients, you can use torch.no_grad() to de-activate the autograd engine.\n",
    "* This will reduce memory usage and speed up computations but you wont be able to backprop (which you dont want in an eval script)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(28.)\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "element 0 of tensors does not require grad and does not have a grad_fn",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-dbf5ed6d017d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# the forward pass i.e. the value of f is calculated as normal\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# the backward pass will throw an error\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/miniconda3/lib/python3.7/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m    183\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    184\u001b[0m         \"\"\"\n\u001b[0;32m--> 185\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    186\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    187\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.7/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m    125\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m    126\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 127\u001b[0;31m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m    128\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    129\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: element 0 of tensors does not require grad and does not have a grad_fn"
     ]
    }
   ],
   "source": [
    "x = torch.arange(4.0)\n",
    "x.requires_grad_(True)\n",
    "with torch.no_grad():\n",
    "    f = 2*torch.dot(x, x)\n",
    "print(f) # the forward pass i.e. the value of f is calculated as normal\n",
    "f.backward() # the backward pass will throw an error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WBM9h-oVd1ek",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Summary\n",
    "\n",
    "* A derivative can be interpreted as the instantaneous rate of change of a function with respect to its variable. \n",
    "* It is also the slope of the tangent line to the curve of the function.\n",
    "* A gradient is a vector whose components are the partial derivatives of a multivariate function with respect to all its variables.\n",
    "* The chain rule enables us to differentiate composite functions.\n",
    "* Deep learning frameworks overcome this problem by automatically calculating derivatives, by *automatic differentiation*.\n",
    "* Automatic differentiation enables the system to subsequently calculate gradients.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 854,
     "status": "ok",
     "timestamp": 1610905982154,
     "user": {
      "displayName": "Georgios Tzimiropoulos",
      "photoUrl": "",
      "userId": "03131831684678803743"
     },
     "user_tz": 0
    },
    "id": "hzYvXpcSd1ek",
    "outputId": "2cbc7f12-8b5c-40df-a88b-938817b23d26",
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(32.7998, grad_fn=<SumBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# Another example\n",
    "# Input feature vector x with 10 features\n",
    "x = torch.rand(10, 1)\n",
    "x.requires_grad_(True)\n",
    "# From x we want to learn to predict 5 values stored in output vector y\n",
    "y = torch.rand(5, 1)\n",
    "#print(y)\n",
    "# Our model is a matrix W with 5 rows and 10 columns. \n",
    "W = torch.rand(5, 10)\n",
    "W.requires_grad_(True)\n",
    "# Each row of the model will be multiplied with x giving a single output. Overall the model produces 5 outputs.\n",
    "# The model's predictions are y_hat = W*x which is 5-dim vector\n",
    "y_hat = torch.mm(W, x) #also can use: y_hat = W@x\n",
    "#print(y_hat)\n",
    "# To improve the model we must make y_hat close to y  \n",
    "# We can minimize the distance between y_hat and y using the (squared) L2 norm\n",
    "L = ((y_hat-y)**2).sum()\n",
    "# L is a multivariable function like the ones we saw above! \n",
    "# It depends on both x=(x1,...,x10) and W=(w11,...,w5,10)\n",
    "# For this specific input and model parameters the value is:\n",
    "print(L)\n",
    "# To update the model, we need to update model parameters wij.\n",
    "# To update w11 we need to calculate ThetaL/Thetaw11. \n",
    "# ThetaL/Thetaw11 in general is also a function of both x=(x1,...,x10) and W=(w11,...,w5,10). \n",
    "# We need to evaluate it for a given input x, current model parameters, outputs y and y_hat.\n",
    "# Then wij_new = wij_new - ThetaL/Thetawij"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 540,
     "status": "ok",
     "timestamp": 1610905984134,
     "user": {
      "displayName": "Georgios Tzimiropoulos",
      "photoUrl": "",
      "userId": "03131831684678803743"
     },
     "user_tz": 0
    },
    "id": "7tE46T4Wd1ek",
    "outputId": "44c2c670-1c34-4f70-f347-0b6c59daf925",
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[0.3663],\n",
       "         [0.4006],\n",
       "         [0.0853],\n",
       "         [0.7513],\n",
       "         [0.3562],\n",
       "         [0.4861],\n",
       "         [0.2470],\n",
       "         [0.5803],\n",
       "         [0.8117],\n",
       "         [0.8044]]), None)"
      ]
     },
     "execution_count": 10,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.data, x.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 527,
     "status": "ok",
     "timestamp": 1610905985387,
     "user": {
      "displayName": "Georgios Tzimiropoulos",
      "photoUrl": "",
      "userId": "03131831684678803743"
     },
     "user_tz": 0
    },
    "id": "WA1yPLFvd1el",
    "outputId": "3e193cb6-6766-4ff1-cb42-876453af20c2",
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[0.3205, 0.5586, 0.3583, 0.6945, 0.3542, 0.8456, 0.3758, 0.8280, 0.0752,\n",
       "          0.0609],\n",
       "         [0.9842, 0.7040, 0.4851, 0.3832, 0.8733, 0.8766, 0.2195, 0.9017, 0.8032,\n",
       "          0.4650],\n",
       "         [0.1897, 0.0332, 0.8849, 0.4892, 0.1550, 0.5171, 0.9341, 0.8754, 0.0963,\n",
       "          0.6587],\n",
       "         [0.8442, 0.3705, 0.8403, 0.6849, 0.6666, 0.9858, 0.7914, 0.1854, 0.7960,\n",
       "          0.7480],\n",
       "         [0.6566, 0.4023, 0.2218, 0.6762, 0.0688, 0.6223, 0.8890, 0.9557, 0.9012,\n",
       "          0.6282]]), None)"
      ]
     },
     "execution_count": 11,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "W.data, W.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "executionInfo": {
     "elapsed": 527,
     "status": "ok",
     "timestamp": 1610905986637,
     "user": {
      "displayName": "Georgios Tzimiropoulos",
      "photoUrl": "",
      "userId": "03131831684678803743"
     },
     "user_tz": 0
    },
    "id": "ZupmowXsd1el",
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "L.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 829,
     "status": "ok",
     "timestamp": 1610905988708,
     "user": {
      "displayName": "Georgios Tzimiropoulos",
      "photoUrl": "",
      "userId": "03131831684678803743"
     },
     "user_tz": 0
    },
    "id": "D5G4gK-pd1el",
    "outputId": "06d69a40-89dd-45c4-a028-5bb65c84a4e9",
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[0.3663],\n",
       "         [0.4006],\n",
       "         [0.0853],\n",
       "         [0.7513],\n",
       "         [0.3562],\n",
       "         [0.4861],\n",
       "         [0.2470],\n",
       "         [0.5803],\n",
       "         [0.8117],\n",
       "         [0.8044]]), tensor([[16.3076],\n",
       "         [10.8437],\n",
       "         [13.8839],\n",
       "         [14.8413],\n",
       "         [11.2648],\n",
       "         [19.7884],\n",
       "         [16.2133],\n",
       "         [18.4138],\n",
       "         [15.0485],\n",
       "         [13.4538]]))"
      ]
     },
     "execution_count": 13,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.data, x.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 476,
     "status": "ok",
     "timestamp": 1610905989977,
     "user": {
      "displayName": "Georgios Tzimiropoulos",
      "photoUrl": "",
      "userId": "03131831684678803743"
     },
     "user_tz": 0
    },
    "id": "IbzZb-pgd1el",
    "outputId": "ff4002c1-9e22-49b1-d150-c521df7a87a5",
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[0.3205, 0.5586, 0.3583, 0.6945, 0.3542, 0.8456, 0.3758, 0.8280, 0.0752,\n",
       "          0.0609],\n",
       "         [0.9842, 0.7040, 0.4851, 0.3832, 0.8733, 0.8766, 0.2195, 0.9017, 0.8032,\n",
       "          0.4650],\n",
       "         [0.1897, 0.0332, 0.8849, 0.4892, 0.1550, 0.5171, 0.9341, 0.8754, 0.0963,\n",
       "          0.6587],\n",
       "         [0.8442, 0.3705, 0.8403, 0.6849, 0.6666, 0.9858, 0.7914, 0.1854, 0.7960,\n",
       "          0.7480],\n",
       "         [0.6566, 0.4023, 0.2218, 0.6762, 0.0688, 0.6223, 0.8890, 0.9557, 0.9012,\n",
       "          0.6282]]),\n",
       " tensor([[1.5049, 1.6456, 0.3502, 3.0863, 1.4630, 1.9970, 1.0149, 2.3838, 3.3345,\n",
       "          3.3043],\n",
       "         [2.0344, 2.2245, 0.4734, 4.1721, 1.9778, 2.6996, 1.3719, 3.2225, 4.5077,\n",
       "          4.4669],\n",
       "         [1.4295, 1.5631, 0.3327, 2.9317, 1.3898, 1.8969, 0.9640, 2.2644, 3.1675,\n",
       "          3.1388],\n",
       "         [2.1762, 2.3796, 0.5065, 4.4630, 2.1157, 2.8878, 1.4676, 3.4472, 4.8220,\n",
       "          4.7783],\n",
       "         [2.1036, 2.3002, 0.4896, 4.3141, 2.0451, 2.7914, 1.4186, 3.3322, 4.6611,\n",
       "          4.6189]]))"
      ]
     },
     "execution_count": 14,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "W.data, W.grad"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "colab": {
   "name": "Automatic_Differentiation.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
