{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VufGKw_omDyI"
   },
   "outputs": [],
   "source": [
    "# Setting up google drive \n",
    "from google.colab import drive\n",
    "drive.mount('/content/gdrive', force_remount=True)\n",
    "import sys\n",
    "sys.path.append('/content/gdrive/MyDrive/Colab Notebooks')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZnGyMHwnmDyN"
   },
   "outputs": [],
   "source": [
    "import my_utils as mu\n",
    "import torch\n",
    "from torch import nn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9qIcycuzmDyN"
   },
   "source": [
    "# The Task\n",
    "\n",
    "* Our goal for this week is to write some code to create an MLP and then experiment with several training options and hyper-parameters.\n",
    "\n",
    "* For this reason we will use again the same pipeline we used last week! \n",
    "    \n",
    "* The Learning Outcome: Hands-on application of PyTorch's API for creating and training MLPs for classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LyLnTr_AmDyN"
   },
   "outputs": [],
   "source": [
    "# Read training and test data\n",
    "batch_size = 256\n",
    "train_iter, test_iter = mu.load_data_fashion_mnist(batch_size)\n",
    "# type(train_iter)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8T4KhkZ8mDyO",
    "origin_pos": 4
   },
   "source": [
    "## Task 1\n",
    "\n",
    "\n",
    "* The model below implements Softmax Regression. It has 1 Linear (or Fully-Connected Layer). Expand the `Net` class to have one more linear layer as follows: the 1st Linear layer will output `num_hidden` outputs. Set `num_hidden=10`.  Moreover, apply a `ReLU` activation function to the output of the 1st Linear layer. The second linear layer will have `num_hidden` inputs and 10 outputs (i.e. equal to the number of classes).\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JkOsdPzxmDyO"
   },
   "outputs": [],
   "source": [
    "class Net(torch.nn.Module):\n",
    "    def __init__(self, num_inputs, num_outputs):\n",
    "        super(Net, self).__init__()\n",
    "        self.num_inputs = num_inputs\n",
    "        self.num_outputs = num_outputs\n",
    "        self.Linear1 = nn.Linear(num_inputs, num_outputs)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = x.view(-1, self.num_inputs)\n",
    "        out = self.Linear1(x)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "O-ERE3PZmDyO",
    "origin_pos": 6,
    "tab": [
     "pytorch"
    ]
   },
   "outputs": [],
   "source": [
    "# Model instantiation and initialisation \n",
    "def init_weights(m):\n",
    "    if type(m) == nn.Linear: # by checking type we can init different layers in different ways\n",
    "        torch.nn.init.normal_(m.weight, std=0.01)\n",
    "        torch.nn.init.zeros_(m.bias)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zbicePCgmDyP"
   },
   "outputs": [],
   "source": [
    "# Create and initialize your model here:\n",
    "num_inputs, num_hidden, num_outputs = 784, 256, 10\n",
    "net = ??\n",
    "\n",
    "# Initialise your model here using init_weights function. If not done, default initiliazation will be used."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gtMsRdogmDyP",
    "origin_pos": 8
   },
   "source": [
    "# Loss and Optimization Algorithm\n",
    "* As in Softmax Regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7puq_4UkmDyP",
    "origin_pos": 10,
    "tab": [
     "pytorch"
    ]
   },
   "outputs": [],
   "source": [
    "# Creare your loss here. Use Cross Entropy loss:\n",
    "loss = ??\n",
    "lr, wd = 0.1, 0\n",
    "# Creare your optimizer here. Use SGD with weight decay wd and learning rate lr.\n",
    "optimizer = ??"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LxWGfOW0mDyP"
   },
   "source": [
    "# Training\n",
    "\n",
    "* Use `my_utils.train_ch3` as in Softmax Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BgZLHwPAmDyQ",
    "origin_pos": 12,
    "tab": [
     "pytorch"
    ]
   },
   "outputs": [],
   "source": [
    "num_epochs = 60\n",
    "mu.train_ch3(net, train_iter, test_iter, loss, num_epochs, optimizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cazx_oGmmDyQ"
   },
   "source": [
    "## Further Tasks\n",
    "\n",
    "* Explore the following training options and observe their impact on the evolution of the **training loss**, the **training accuracy** and the **validation accuracy**. Different choices might have different impact on both accuracy and convergence:\n",
    "    1. Change the number of hidden layers to 256\n",
    "    1. Investigate different learning rates. Use 0.5, 0.9 and 0.01.\n",
    "    1. Investigate adding weight decay wd=0.0005\n",
    "    1. Investigate `Sigmoid` activation function. \n",
    "    1. Try different schemes for initializing the weights. "
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Week05_Lab.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
