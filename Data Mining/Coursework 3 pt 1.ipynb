{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2050d048",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from mlxtend.preprocessing import TransactionEncoder\n",
    "from mlxtend.frequent_patterns import apriori\n",
    "from mlxtend.frequent_patterns import association_rules"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7e466fc",
   "metadata": {},
   "source": [
    "# ECS766 Coursework 2 - Elliot Linsey"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f81be2c",
   "metadata": {},
   "source": [
    "## Q1:\n",
    "\n",
    "Computing the support of every subset of an itemset can lead to numbers too large for a computer to compute or store. This is because if we have a large itemset, then we have to compute every combination of items within to calculate every support. The equation to calculate the number of itemsets within a large itemset is $2^n-1$, with n being the number of items within your itemset. If we had an itemset of 100 items, the equation would be $2^{100}-1 \\approx 1.27\\times10^{30}$, this resulting number is too large to be handled by the computer.  \n",
    "\n",
    "The apriori algorithm is used to reduce the amount of calculations involved by using apriori knowledge that the subset of a frequent itemset is also frequent. Therefore, if an itemset in $L_k$ is frequent then the corresponding subsets are also frequent. Using this knowledge, it can find the initial candidate itemsets, $C_1$, use this to find the frequent itemsets $L_1$ by comparing with the previously decided support count. Then, generate the next set of candidates $C_2$, or superset, of the original candidates by joining $L_1 \\Join L_1$. After joining, you create $L_2$ by pruning the candidates of $C_2$ that lack frequent itemsets. As you are removing infrequent itemsets as you go through the algorithm, you do not need to calculate every single itemset support as you only keep the itemsets that meet your predetermined support level. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1de1d662",
   "metadata": {},
   "source": [
    "## Q2:\n",
    "\n",
    "If $L_1$ contains the initial frequent itemsets, then every $k\\geq2$ must be a superset of $L_1$ because the apriori rule has the attribute of antimonotonicity. This refers to the fact that if a set cannot pass the support test, then all supersets will also fail. Therefore, for all sets in $k\\geq2$, they must be made from an itemset of $L_1$ (that passes the support test), otherwise they too will not pass the support test. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "539ec80f",
   "metadata": {},
   "source": [
    "## Q3:\n",
    "\n",
    "$C_3$ = {{1,2,4},{2,3,4}}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a11afe4",
   "metadata": {},
   "source": [
    "## Q4:\n",
    "\n",
    "As $S_2$ is a subset of $S_1$, therefore the support of $S_2$ will be $\\geq$ the support of $S_1$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19b50897",
   "metadata": {},
   "source": [
    "## Q5:\n",
    "\n",
    "0.8???"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbf09fef",
   "metadata": {},
   "source": [
    "## Q6:\n",
    "\n",
    "6"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f222af7",
   "metadata": {},
   "source": [
    "poop"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
