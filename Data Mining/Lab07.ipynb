{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab session 7: Outlier Detection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "The purpose of this lab session is to provide you with an opportunity to gain experience in **outlier detection**, covered in week 9, using common Python libraries.\n",
    "\n",
    "- This lab is the second part of a **two-week assignment** that covers weeks 8 and 9, which is due on **Friday 3rd December 10am**.\n",
    "- The assignment will account for 10% of your overall grade. Questions in this lab sheet will contribute to 5% of your overall grade; questions in the lab sheet for week 8 will cover for another 5% of your overall grade.\n",
    "- <font color = 'maroon'>The last section of this notebook includes the questions that are assessed towards your final grade.</font> \n",
    "\n",
    "This session starts with a tutorial that uses examples to introduce you to the practical knowledge that you will need for the corresponding assignment. We highly recommend that you read the following tutorials if you need a gentler introduction to the libraries that we use:\n",
    "- [Numpy quickstart tutorial](https://numpy.org/devdocs/user/quickstart.html)\n",
    "- [Pandas](https://pandas.pydata.org/pandas-docs/stable/user_guide/10min.html)\n",
    "- [Matplotlib](https://matplotlib.org/tutorials/introductory/pyplot.html)\n",
    "- [Scikit-learn](https://scikit-learn.org/stable/tutorial/basic/tutorial.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Important notes about the assignment: \n",
    "\n",
    "- **PLAGIARISM** <ins>is an irreversible non-negotiable failure in the course</ins> (if in doubt of what constitutes plagiarism, ask!). \n",
    "- The total assessed coursework is worth 40% of your final grade.\n",
    "- There will be 9 lab sessions and 4 assignments.\n",
    "- One assignment will cover 2 consecutive lab sessions and will be worth 10 marks (percentages of your final grade).\n",
    "- The submission cut-off date will be 7 days after the deadline and penalties will be applied for late submissions in accordance with the School policy on late submissions.\n",
    "- You are asked to submit a **report** that should answer the questions specified in the last section of this notebook. The report should be in a **single PDF document** (so **NOT** *doc, docx, notebook* etc). This single PDF document will include your answers to both the week 8 and week 9 labs.\n",
    "- No other means of submission other than submitting your assignment through the appropriate QM+ link are acceptable at any time. Submissions sent via email will **not** be considered.\n",
    "- Please name your report as follows: Assignment3-StudentName-StudentNumber.pdf\n",
    "- Cases of **Extenuating Circumstances (ECs)** have to go through the proper procedure of the School in due time. Only cases approved by the School in due time can be considered."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Outlier detection using parametric methods \n",
    "\n",
    "This approach assumes that the majority of the data instances are governed by some well-known probability distribution, e.g. a Gaussian distribution. Outliers can then be detected by seeking for observations that do not fit the overall distribution of the data. \n",
    "\n",
    "In this example, our goal is to detect anomalous changes in the daily closing prices of various stocks. The input data **stocks.csv** (available in the lab supplementary material) contains the historical closing prices of stocks for 3 large corporations (Microsoft, Ford Motor Company, and Bank of America). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load CSV file, set the 'Date' values as the index of each row, and display the first rows of the dataframe\n",
    "stocks = pd.read_csv('stocks.csv', header='infer') \n",
    "stocks.index = stocks['Date']\n",
    "stocks = stocks.drop(['Date'],axis=1)\n",
    "stocks.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can compute the percentage of changes in the daily closing price of each stock as follows:\n",
    "\\begin{equation}\n",
    "\\Delta(t) = 100 \\times \\frac{x_t - x_{t-1}}{x_{t-1}} \n",
    "\\end{equation}\n",
    "\n",
    "where $x_t$ denotes the price of a stock on day $t$ and $x_{t-1}$ denotes the price on its previous day, $t-1$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "N,d = stocks.shape\n",
    "# Compute delta, which denotes the percentage of changes in the daily closing price of each stock\n",
    "delta = pd.DataFrame(100*np.divide(stocks.iloc[1:,:].values-stocks.iloc[:N-1,:].values, stocks.iloc[:N-1,:].values),\n",
    "                    columns=stocks.columns, index=stocks.iloc[1:].index)\n",
    "delta.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now plot the distribution of the percentage daily changes in stock price as a 3-dimensional scatter plot:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "fig = plt.figure(figsize=(8,5)).gca(projection='3d')\n",
    "fig.scatter(delta.MSFT,delta.F,delta.BAC)\n",
    "fig.set_xlabel('Microsoft')\n",
    "fig.set_ylabel('Ford')\n",
    "fig.set_zlabel('Bank of America')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's assume that the data follows a multivariate (i.e. multidimensional) Gaussian distribution. Such a probability distribution can be characterised by two statistics: the mean and covariance matrix of the 3-dimensional data. \n",
    "\n",
    "We can then compute the mean and covariance matrix of the 3-dimensional 'delta' data (which represent the percentage of changes in the daily closing price of each stock). Then, as a distance measure, to determine the anomalous trading days, we can compute the **Mahalanobis distance** (to be more precise, the square of the Mahalanobis distance) between the percentage of price change on each day against the mean percentage of price change:\n",
    "\\begin{equation}\n",
    "\\textrm{MDist}(x,\\bar{x}) = (x - \\bar{x})^T S^{-1}(x - \\bar{x})\n",
    "\\end{equation}\n",
    "where $x$ is assumed to be a row vector, $\\bar{x}$ denotes the mean vector, and $S$ denotes the covariance matrix of the data.\n",
    "\n",
    "See Section 12.3 in the \"Data Mining: Concepts and Techniques\" book for more information on the Mahalanobis distance. As a first step, we can define a function for the Mahalanobis distance:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy.linalg import inv\n",
    "\n",
    "def mahalanobis(x=None, data=None):\n",
    "    \"\"\"Compute the Mahalanobis Distance between each row of x and the data  \n",
    "    x    : vector or matrix of data with, say, p columns.\n",
    "    data : ndarray of the distribution from which Mahalanobis distance of each observation of x is to be computed.\n",
    "    \"\"\"\n",
    "    x_mu = x - np.mean(data)\n",
    "    cov = np.cov(data.values.T)\n",
    "    inv_covmat = np.linalg.inv(cov)\n",
    "    left = np.matmul(x_mu, inv_covmat)\n",
    "    mahal = np.dot(left, x_mu.T)\n",
    "    return mahal.diagonal()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, we can call the created function for the Mahalanobis distance on the 'delta' dataframe containing the daily percentage changes for each stock:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute Mahalanobis distance for delta dataset\n",
    "mahal = mahalanobis(x=delta, data=delta[['MSFT', 'F', 'BAC']])\n",
    "\n",
    "# Assign an outlier score for the data based on the computed Mahalanobis distance\n",
    "outlier_score = mahal\n",
    "\n",
    "# Display 3D scatterplot with datapoints having a different color according to their outlier score\n",
    "fig = plt.figure(figsize=(10,6))\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "p = ax.scatter(delta.MSFT,delta.F,delta.BAC,c=outlier_score,cmap='jet')\n",
    "ax.set_xlabel('Microsoft')\n",
    "ax.set_ylabel('Ford')\n",
    "ax.set_zlabel('Bank of America')\n",
    "fig.colorbar(p)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The top outliers are shown as the dark red and orange points in the above scatterplot. We can examine the dates associated with the top-5 highest outlier scores as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outlier = pd.DataFrame(outlier_score, index=delta.index, columns=['Outlier score'])\n",
    "result = pd.concat((delta,outlier), axis=1)\n",
    "result.nlargest(5,'Outlier score')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see for example that the top outlier was detected for 13th October 2008. This was a period of economic instability due to the 2008 global financial crisis (https://en.wikipedia.org/wiki/Global_financial_crisis_in_October_2008).\n",
    "\n",
    "We can subsequently inspect stocks around the top-2 outlier dates for each company, and see to which compani(es) are these outliers attributed to."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "fig, (ax1,ax2) = plt.subplots(nrows=1, ncols=2, figsize=(15,6))\n",
    "\n",
    "ts = delta[445:452]\n",
    "ts.plot.line(ax=ax1)\n",
    "ax1.set_xticks(range(7))\n",
    "ax1.set_xticklabels(ts.index)\n",
    "ax1.set_ylabel('Percent Change')\n",
    "\n",
    "ts = delta[477:484]\n",
    "ts.plot.line(ax=ax2)\n",
    "ax2.set_xticks(range(7))\n",
    "ax2.set_xticklabels(ts.index)\n",
    "ax2.set_ylabel('Percent Change')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Outlier detection using proximity-based approaches\n",
    "\n",
    "This is a model-free outlier detection approach as it does not require constructing an explicit model of the normal class to determine the outlier score of data instances. The example code shown below employs the k-nearest neighbor approach to calculate the outlier score. Specifically, a normal instance is expected to have a small distance to its k-th nearest neighbour whereas an outlier is likely to have a large distance to its k-th nearest neighbour. In the example below, we apply the distance-based approach with k=4 to identify the anomalous trading days from the stock market data described in the previous section.\n",
    "\n",
    "For more information on the NearestNeighbors() function please see the scikit learn documnetation: https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.NearestNeighbors.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import NearestNeighbors\n",
    "import numpy as np\n",
    "from scipy.spatial import distance\n",
    "\n",
    "# Implement a k-nearest neighbour approach using k=4 neighbours\n",
    "knn = 4\n",
    "nbrs = NearestNeighbors(n_neighbors=knn, metric=distance.euclidean).fit(delta.values)\n",
    "distances, indices = nbrs.kneighbors(delta.values)\n",
    "\n",
    "# The outlier score is set as the distance between the point and its k-th nearest neighbour\n",
    "outlier_score = distances[:,knn-1]\n",
    "\n",
    "# Plot 3D scatterplot of outlier scores\n",
    "fig = plt.figure(figsize=(10,6))\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "p = ax.scatter(delta.MSFT,delta.F,delta.BAC,c=outlier_score,cmap='jet')\n",
    "ax.set_xlabel('Microsoft')\n",
    "ax.set_ylabel('Ford')\n",
    "ax.set_zlabel('Bank of America')\n",
    "fig.colorbar(p)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The results are slightly different than the one shown in Section 1 due to the difference distance used (Euclidean distance vs Mahalanobis distance) and the proximity criterion to detect the outliers. \n",
    "\n",
    "We can examine the dates associated with the top-5 highest outlier scores as follows. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outlier = pd.DataFrame(outlier_score, index=delta.index, columns=['Outlier score'])\n",
    "result = pd.concat((delta,outlier), axis=1)\n",
    "result.nlargest(5,'Outlier score')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, similarly to what was carried out in Section 1, we can inspect stocks around those outlier dates for each company. The below figure inspects the delta values for each company around the date of the 3rd detected outlier, on 7th October 2008, which represents a key date for the 2008 financial resession with large drops in stock values. Two companies seem to be primarily responsible for the creation of this outlier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(10,4))\n",
    "\n",
    "ax = fig.add_subplot(111)\n",
    "ts = delta[440:447]\n",
    "ts.plot.line(ax=ax)\n",
    "ax.set_xticks(range(7))\n",
    "ax.set_xticklabels(ts.index)\n",
    "ax.set_ylabel('Percent Change')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Outlier detection using classification-based methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The **support vector machine (SVM)** algorithm developed initially for binary classification can be used for one-class classification, and therefore for outlier detection using a classification-based method where we construct a classifier to describe only the normal class.\n",
    "\n",
    "When modeling one class, the algorithm captures the density of the majority class and classifies examples on the extremes of the density function as outliers. This modification of SVM is referred to as **One-Class SVM**.\n",
    "\n",
    "In this section, we will work with a different dataset on house prices available at: https://raw.githubusercontent.com/jbrownlee/Datasets/master/housing.csv . This dataset has 13 input variables that describe the properties of the house and suburb and requires the prediction of the median value of houses in the suburb in thousands of dollars. Information and metadata about the dataset can be found at: https://raw.githubusercontent.com/jbrownlee/Datasets/master/housing.names . Please spend some time to inspect the dataset and its metadata.\n",
    "\n",
    "As a first step, we load the dataset and split it into input and output (target) elements:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pandas import read_csv\n",
    "\n",
    "# Loading the dataset\n",
    "url = 'https://raw.githubusercontent.com/jbrownlee/Datasets/master/housing.csv'\n",
    "df = read_csv(url, header=None)\n",
    "\n",
    "# Extracting the values from the dataframe\n",
    "data = df.values\n",
    "\n",
    "# Split dataset into input and output elements\n",
    "X, y = data[:, :-1], data[:, -1]\n",
    "\n",
    "# Summarize the shape of the dataset\n",
    "print(X.shape, y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the OneClassSVM() function in scikit-learn, we can initialise and train an one-class SVM classifier on the input data. Please study the [OneClassSVM() documentation](https://scikit-learn.org/stable/modules/generated/sklearn.svm.OneClassSVM.html) for information on input arguments. \n",
    "\n",
    "We can then print the estimated labels, which for each object are -1 for outliers and 1 for inliers (i.e. data points that are considered normal)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import OneClassSVM\n",
    "\n",
    "ee = OneClassSVM(nu=0.01,gamma='auto')\n",
    "yhat = ee.fit_predict(X) # Perform fit on input data and returns labels for that input data.\n",
    "\n",
    "print(yhat) # Print labels: -1 for outliers and 1 for inliers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Having trained the one-class SVM, we can then select all rows from the dataset that are **not** outliers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select all rows that are not outliers\n",
    "mask = yhat != -1\n",
    "X, y = X[mask, :], y[mask]\n",
    "\n",
    "# Summarize the shape of the updated dataset\n",
    "print(X.shape, y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we see, the new dataset has a significantly smaller number of objects, all of which are considered by the one-class SVM to be `inliers', i.e. to belong to the normal distribution of the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color = 'maroon'>Assignment 3 [Part 2 of 2]</font>\n",
    "\n",
    "For your answers to the assignment, please include include your workings (e.g. equations, code) when this is relevant to the question. Questions 1-2 are pen-and paper exercises. Question 3 can be addressed either on paper or using code. Questions 4-5 are coding exercises.\n",
    "\n",
    "1. For an application on credit card fraud detection, we are interested in detecting contextual outliers. Suggest 2 possible contextual attributes and 2 possible behavioural attributes that could be used for this application, and explain why each of your suggested attribute should be considered as either contextual or behavioural. [1 mark out of 5]\n",
    "\n",
    "2. Assume that you are provided with the [University of Wisconsin breast cancer dataset](https://archive.ics.uci.edu/ml/machine-learning-databases/breast-cancer-wisconsin/breast-cancer-wisconsin.data) from the Week 3 lab, and that you are asked to detect outliers from this dataset. Additional information on the dataset attributes can be found [online](https://archive.ics.uci.edu/ml/machine-learning-databases/breast-cancer-wisconsin/breast-cancer-wisconsin.names). Explain one possible outlier detection method that you could apply for detecting outliers for this particular dataset, explain what is defined as an outlier for your suggested approach given this particular dataset, and justify why would you choose this particular method for outlier detection. [1 mark out of 5]\n",
    "\n",
    "3. The monthly rainfall in the London borough of Tower Hamlets in 2019 had the following amount of precipitation (measured in mm, values from January-December 2018): {22.93, 20.69, 25.75, 23.84, 25.34, 3.25, 23.55, 28.28, 23.72, 22.42, 26.83, 23.82}. Assuming that the data is based on a normal distribution, identify outlier values in the above dataset using the maximum likelihood method. [1 mark out of 5]\n",
    "\n",
    "4. Using the stock prices dataset used in sections 1 and 2 of this lab notebook, estimate the outliers in the dataset using the one-class SVM classifier approach. As input to the classifier, use the percentage of changes in the daily closing price of each stock, as was done in section 1 of the notebook. Plot a 3D scatterplot of the dataset, where each object is color-coded according to whether it is an outlier or an inlier. Also compute a histogram and the frequencies of the estimated outlier and inlier labels. In terms of the plotted results, how does the one-class SVM approach for outlier detection differ from the parametric and proximity-based methods used in the lab notebook? What percentage of the dataset objects are classified as outliers? [1 mark out of 5]\n",
    "\n",
    "5. This question will combine concepts from both data preprocessing and outlier detection. Using the house prices dataset from Section 3 of this lab notebook, perform dimensionality reduction on the dataset using PCA with 2 principal components (make sure that the dataset is z-score normalised beforehand, and remember that PCA should only be applied on the input attributes). Then, perform outlier detection on the pre-processed dataset using the k-nearest neighbours approach using k=2. Display a scatterplot of the two principal components, where each object is colour-coded according to the computed outlier score. [1 marks out of 5]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
