{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab Work: Testing\n",
    "\n",
    "## How not to write code\n",
    "\n",
    "Here is a made-up example combining various ways of not writing clean code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Coefficients: alpha=1.000, beta=0.000\n"
     ]
    }
   ],
   "source": [
    "def function1():\n",
    "    global f1\n",
    "    f1 = sum(values) / len(values)\n",
    "\n",
    "def function2(x, mean_x, y, mean_y):\n",
    "    return sum([(x_i - mean_x) * (y_i - mean_y) for x_i, y_i in zip(x, y)])\n",
    "\n",
    "def variance():\n",
    "    return calcx1**2 + calcx2**2 + calcx3**2\n",
    "\n",
    "def compute_alpha_beta(x_values, y_values):\n",
    "    global values\n",
    "    values = x_values\n",
    "    function1()\n",
    "    result1 = f1\n",
    "    values=        y_values\n",
    "    function1()\n",
    "    result2=f1\n",
    "    cov = function2(x_values, result1, y_values, result2)\n",
    "    global calcx1\n",
    "    calcx1 =             x_values[0] - result1\n",
    "    global                  calcx2\n",
    "    calcx2 = x_values[1] -result1\n",
    "    global calcx3\n",
    "    calcx3 = x_values[2] - result1\n",
    "    alpha = cov / variance()\n",
    "    beta = result2 - alpha* result1\n",
    "    return [alpha,beta]\n",
    "\n",
    "alpha, beta = compute_alpha_beta([1, 2, 3], [1, 2, 3])\n",
    "print('Coefficients: alpha=%.3f, beta=%.3f' % (alpha, beta))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task\n",
    "\n",
    "If you're on campus, discuss the above implementation with your the person sitting next to you. Identify as many instances of poor programming practice as possible.\n",
    "\n",
    "If you're participating online, write down all criticism and then post it in the lab channel on MS Teams."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pragmatic testing\n",
    "Your testing should help find bugs in the implementation, but you should also make sure your testing itself isn't flawed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Coefficients: alpha=0.03226, beta=5.58065\n"
     ]
    }
   ],
   "source": [
    "alpha, beta = compute_alpha_beta([0, -2, 10], [5, 6, 6])\n",
    "print('Coefficients: alpha=%.5f, beta=%.5f' % (alpha, beta))\n",
    "assert round(alpha, 3) == 0.032 and round(beta, 3) == 5.581\n",
    "\n",
    "# Data points on a line should yield an exact result\n",
    "alpha, beta = compute_alpha_beta([1, 2, 3], [2, 4, 6])\n",
    "assert alpha == 2 and beta == 0\n",
    "\n",
    "# A slope of 0 should be ok\n",
    "alpha, beta = compute_alpha_beta([1, 2, 3], [4, 4, 4])\n",
    "assert alpha == 0 and beta == 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task\n",
    "\n",
    "Review your own testing practice as done for Coursework 1, and consider whether you have ways of making sure your tests actually work as expected. The above examples may serve as some ideas, but don't feel bound to these examples. Coursework 2 requires that you sanity check your tests, and this exercise should prepare you for doing so. Feel free to apply this to either Coursework 1 (linear regression), or Coursework 2, if you have already made sufficient progress on it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (Unit-)Testing with Classes\n",
    "\n",
    "The following code implements binary search using a class. This class contains a number of helper methods. In this case, unit testing is very well defined: tests are set up for each method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "\n",
    "class binary_search:\n",
    "    def __init__(self, A):\n",
    "        self.A = A\n",
    "        self.counter = 0\n",
    "        \n",
    "    def get_list(self):\n",
    "        return self.A\n",
    "        \n",
    "    def search(self, value):\n",
    "        return self.binary_search(start=0, end=len(self.A), value=value)\n",
    "    \n",
    "    def size_of_array(self):\n",
    "        return len(self.A)\n",
    "    \n",
    "    def print_stats(self):\n",
    "        return \"Invocation count: \" + str(self.counter)\n",
    "        \n",
    "    def reset_stats(self):\n",
    "        self.counter = 0\n",
    "    \n",
    "    def binary_search(self, start, end, value):\n",
    "        self.counter += 1\n",
    "        if start >= end:\n",
    "            # end of search reached\n",
    "            return False\n",
    "\n",
    "        # compute the mid point between start and end\n",
    "        mid = (end - start) // 2 + start\n",
    "\n",
    "        if self.A[mid] == value:\n",
    "            # value found\n",
    "            return True\n",
    "        # recursive calls\n",
    "        elif self.A[mid] > value:\n",
    "            return self.binary_search(start=start, end=mid, value=value)\n",
    "        else:\n",
    "            return self.binary_search(start=mid + 1, end=end, value=value)\n",
    "\n",
    "\n",
    "def unit_test_size_of():\n",
    "    b1 = binary_search(A=[ 2, 42, 55, 78, 100 ])\n",
    "    assert(b1.size_of_array() == 5)\n",
    "    b2 = binary_search(A=[])\n",
    "    assert(b2.size_of_array() == 0)\n",
    "    b3 = binary_search(A=[10])\n",
    "    assert(b3.size_of_array() == 1)\n",
    "        \n",
    "def unit_test_stats():  \n",
    "    b1 = binary_search(A=[ 2, 42, 55, 78, 100 ])\n",
    "    assert(b1.print_stats() == \"Invocation count: 0\")\n",
    "    b1.search(value=42)\n",
    "    assert(b1.print_stats() == \"Invocation count: 2\")\n",
    "    b1.reset_stats()\n",
    "    assert(b1.print_stats() == \"Invocation count: 0\")\n",
    "\n",
    "def unit_test_search():\n",
    "    b1 = binary_search(A=[ 2, 42, 55, 78, 100 ])\n",
    "    assert(b1.search(value=42) == True)\n",
    "    b2 = binary_search(A=[])\n",
    "    assert(b2.search(value=42) == False)\n",
    "    b3 = binary_search(A=[ 2, 42, 55, 78, 100 ])\n",
    "    assert(b3.search(value=1) == False)\n",
    "\n",
    "def unit_test_get_list():\n",
    "    b1 = binary_search(A=[ 2, 42, 55, 78, 100 ])\n",
    "    result = b1.get_list()\n",
    "    assert(len(result) == 5)\n",
    "    assert(result[0] == 2)\n",
    "    assert(result[-1] == 100)\n",
    "    assert(result == [ 2, 42, 55, 78, 100 ])\n",
    "    \n",
    "def unit_tests():\n",
    "    unit_test_size_of()\n",
    "    unit_test_stats()\n",
    "    unit_test_search()\n",
    "    unit_test_get_list()\n",
    "\n",
    "def integration_test():\n",
    "    b1 = binary_search(A=[ 2, 42, 55, 78, 100 ])\n",
    "    assert(b1.size_of_array() == 5)\n",
    "    assert(b1.print_stats() == \"Invocation count: 0\")\n",
    "    assert(b1.search(value=42) == True)\n",
    "    assert(b1.print_stats() == \"Invocation count: 2\")\n",
    "\n",
    "unit_tests()\n",
    "integration_test()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task\n",
    "\n",
    "In an earlier lab session, you created a class-based implementation. Review your testing practice of that implementation and make sure that you have unit test coverage of all methods."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing using Python test frameworks\n",
    "\n",
    "Thus far, all testing was done using functions that were only distinguishable from non-test code by their name (if at all). The actual checks for expected values were done using `assert` statements, and output (if any) was somewhat ad-hoc. In this exercise, you will start using the `unittest` module (https://docs.python.org/3/library/unittest.html). This module provides for test reports.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      ".\n",
      "----------------------------------------------------------------------\n",
      "Ran 1 test in 0.001s\n",
      "\n",
      "OK\n"
     ]
    }
   ],
   "source": [
    "import unittest\n",
    "\n",
    "class test_binary_search(unittest.TestCase):\n",
    "    def test_size_of(self):\n",
    "        b1 = binary_search(A=[ 2, 42, 55, 78, 100 ])\n",
    "        self.assertEqual(b1.size_of_array(), 5)\n",
    "        b2 = binary_search(A=[])\n",
    "        self.assertEqual(b2.size_of_array(), 0)\n",
    "        b3 = binary_search(A=[10])\n",
    "        self.assertEqual(b3.size_of_array(), 1)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    # When not using Jupyter notebooks:\n",
    "    # unittest.main()\n",
    "    # In Jupyter notebooks use this:\n",
    "    unittest.main(argv=['this-is-ignored'], exit=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task\n",
    "\n",
    "Use one of your test harnesses (possibly the one extended in the previous task) and turn it into a `unittest`-based test harness."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
